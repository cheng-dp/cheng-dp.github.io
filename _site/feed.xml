<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-09-17T09:41:09+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">东平的笔记仓库</title><subtitle>我的博客，主要是笔记归档。</subtitle><author><name>Cheng Dongping</name></author><entry><title type="html">Kafka之保证可靠的数据传递</title><link href="http://localhost:4000/2019/04/25/kafka-maintain-credible-data-transport/" rel="alternate" type="text/html" title="Kafka之保证可靠的数据传递" /><published>2019-04-25T00:00:00+08:00</published><updated>2019-04-25T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/25/kafka-maintain-credible-data-transport</id><content type="html" xml:base="http://localhost:4000/2019/04/25/kafka-maintain-credible-data-transport/">&lt;h2 id=&quot;可靠性保证&quot;&gt;可靠性保证&lt;/h2&gt;

&lt;p&gt;Kafka的数据可靠性保证：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;保证分区消息的顺序&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果使用同一个生产者往同一个分区写入消息，消息B在消息A之后写入，Kafka保证B的偏移量比A大，消费者会先读取A再读取B。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;==只有当消息被写入分区的&lt;strong&gt;所有同步副本&lt;/strong&gt;时(不一定要写入磁盘)，它才被认为是&lt;strong&gt;已提交&lt;/strong&gt;的。==&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;生产者可以选择接受不同类型的确认，比如在消息被完成提交时确认，或者被写入首领时确认，或者被发送时就确认。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;只要还有一个副本是活跃的，已经提交的消息就不会丢失。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;消费者只能读取已经提交的消息。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Kafka管理员需要权衡消息存储的可靠性和一致性的重要程度，
以及可用性、高吞吐量、低延迟的硬件成本的重要程度之间的权衡。&lt;/p&gt;

&lt;h2 id=&quot;复制&quot;&gt;复制&lt;/h2&gt;

&lt;p&gt;==Kafka的复制机制和分区的多副本架构是Kafka可靠性保证的核心。==&lt;/p&gt;

&lt;p&gt;Kafka的主题本分为多个分区，分区是基本的数据块。分区存储在单个磁盘上，Kafka保证分区里的消息是有序的。&lt;/p&gt;

&lt;p&gt;每个分区可以有多个副本，其中一个副本是首领副本。所有消息都直接发送给首领副本，或者直接从首领副本读取消息。&lt;/p&gt;

&lt;p&gt;其他副本只需要与首领副本同步，并及时复制最新的消息，当首领副本不可用时，其中一个&lt;strong&gt;同步副本&lt;/strong&gt;将称为新的首领副本。&lt;/p&gt;

&lt;p&gt;跟随者副成称为同步副本的条件：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;与Zookeeper之间有一个活跃的会话。(在过去6s向ZooKeeper发送心跳)&lt;/li&gt;
  &lt;li&gt;在过去10s内从首领那里获取过消息。&lt;/li&gt;
  &lt;li&gt;在过去10s内从首领那里获取过&lt;strong&gt;最新&lt;/strong&gt;的消息。
跟随者由于网络原因成为不同步副本，一旦重新获取最新消息后，可以重新变成同步副本。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;由于Kafka的消息需要所有同步副本确认才能称为&lt;strong&gt;已提交&lt;/strong&gt;，因此，一个滞后的同步副本会降低Kafka的&lt;strong&gt;吞吐率&lt;/strong&gt;。而如果一个副本不再是同步的，就不再会影响Kafka的性能，但是会增大数据风险。&lt;/p&gt;

&lt;h2 id=&quot;broker配置&quot;&gt;broker配置&lt;/h2&gt;

&lt;h3 id=&quot;复制系数&quot;&gt;复制系数&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;replication.factor&lt;/code&gt; : 主题级别的配置参数&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;default.replication.factor&lt;/code&gt; : broker级别的配置参数，配置自动创建的主题&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如果复制系数是N，则每个分区总共会被N个不同的broker复制，总共有N个数据副本。&lt;/p&gt;

&lt;p&gt;建议在要求可用性的场景里把复制系统至少设为3.&lt;/p&gt;

&lt;p&gt;副本的分布也很重要：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;broker.rack&lt;/code&gt; : 为每个broker配置所在的机架
Kafka会保证分区的副本被分布在多个机架上。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;不完全的首领选举&quot;&gt;不完全的首领选举&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;unclean.leader.election&lt;/code&gt;=true/false 默认为true&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当首领不可用时，其他副本都是不同步的，是否允许不同步副本成为新的首领。&lt;/p&gt;

&lt;p&gt;如果不同步的副本不能成为新首领，在旧首领恢复前，Kafka不可用，降低了可用性。&lt;/p&gt;

&lt;p&gt;如果不同步的副本可以成为新首领，因为不同步副本不包括所有旧首领的消息，可能有数据丢失的风险。&lt;/p&gt;

&lt;h3 id=&quot;最少同步副本&quot;&gt;最少同步副本&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;min.insync.replicas&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;消息只有被写入到所有同步副本后才被认为是已提交的，该参数设定此时“所有同步副本”的最少数目。&lt;/p&gt;

&lt;p&gt;对于一个包含3个副本的主题，如果&lt;code class=&quot;highlighter-rouge&quot;&gt;min.insync.replicas&lt;/code&gt;=2，那么至少要存在两个同步副本才向分区写入数据。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;有3个副本，min.insync.replicase=2，如果两个副本不可用，及“所有同步副本” &amp;lt; min.insync.replicase。

broker会停止接受生产者的请求，返回NotEnoughReplicasException.
消费者仍然可以继续读取已有的数据，broker变为只读。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;生产者的可靠性&quot;&gt;生产者的可靠性&lt;/h2&gt;

&lt;p&gt;即使broker配置的尽可能可靠，如果生产者本身是不可靠的，数据丢失仍然会发生。&lt;/p&gt;

&lt;h3 id=&quot;acks设置&quot;&gt;acks设置&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;acks=0&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;此时生产者不管发送是否成功，很大可能会丢失消息。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;acks=1&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;首领收到消息并写入分区文件后即返回确认。&lt;/p&gt;

&lt;p&gt;如果首领在跟随者副本还没有收到更新时崩溃，消息会丢失。&lt;/p&gt;

&lt;p&gt;如果发送消息时，broker正在进行首领选举，生产者会收到LeaderNotAvailableException异常，生产者需要恰当的处理该异常，重发消息。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;acks=all&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;首需要等待所有同步副本都收到消息后才返回确认。&lt;/p&gt;

&lt;p&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;min.insync.replicas&lt;/code&gt;参数结合，决定在返回确认前至少有多少个副本能够收到消息。&lt;/p&gt;

&lt;p&gt;最可靠，但是吞吐率最低。&lt;/p&gt;

&lt;h3 id=&quot;生产者重试&quot;&gt;生产者重试&lt;/h3&gt;

&lt;p&gt;当错误发生时，对于可以自动处理的错误(如，LeaderNotAvailableException)，可以进行多次重试，直至消息发送成功。&lt;/p&gt;

&lt;p&gt;但是，重试可能造成同个消息多次写入的问题，==broker会收到两个相同的消息，Kafka没法保证每个消息只被处理一次。==&lt;/p&gt;

&lt;p&gt;对于幂等消息(如：这个账号里有100美元)，重复消息不会对结果造成影响。但是对于非幂等消息(如：往账号里增加100美元)，会造成结果错误。&lt;/p&gt;

&lt;p&gt;==对于重复消息，可以在消息里加入唯一标识符，并在消费者中进行清理。==&lt;/p&gt;

&lt;h2 id=&quot;消费者的可靠性&quot;&gt;消费者的可靠性&lt;/h2&gt;

&lt;p&gt;只有已经被写入所有==同步副本==的数据，才会被消费者读取，因此消费者得到的消息已经具备了一致性。&lt;/p&gt;

&lt;p&gt;消费者可靠性主要是跟踪哪些消息是已经读取过的，哪些是还没读取过的，保证读取消息时不会丢失。&lt;/p&gt;

&lt;p&gt;如果消费者提交了偏移量，却未能处理完轮询得到的消息，就可能造成消息丢失。&lt;/p&gt;

&lt;h3 id=&quot;消费者的可靠性配置&quot;&gt;消费者的可靠性配置&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;group.id&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果两个消费者具有相同的group.id，并且订阅了同一个主题，每个消费者会分到主题分区的一个子集，也就是只能读取到所有消息的一个子集。&lt;/p&gt;

&lt;p&gt;如果希望消费者可以看到主题的所有消息，需要为它设置唯一的group.id。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;auto.offset.reset = earliest / latest&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;配置在没有偏移量可以提交时，或请求的偏移量在broker上不存在时，消费者的读取位置。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;earliest : 从分区开始位置读取。造成重复读取，不会丢失。&lt;/li&gt;
  &lt;li&gt;latest : 从分区末尾开始读取。可能丢失，不会重复。&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;enable.auto.commit&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;自动提交偏移量。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;auto.commit.interval.ms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;偏移量提交时间间隔，默认为5s。&lt;/p&gt;

&lt;h3 id=&quot;消费者的可靠性-1&quot;&gt;消费者的可靠性&lt;/h3&gt;

&lt;h5 id=&quot;总是在处理完事件后再提交偏移量&quot;&gt;总是在处理完事件后再提交偏移量&lt;/h5&gt;

&lt;p&gt;提交的偏移量应该是处理完成的消息偏移量，而不是读取到的偏移量。&lt;/p&gt;

&lt;h5 id=&quot;偏移量提交频率是性能和重复消息数量之间的权衡&quot;&gt;偏移量提交频率是性能和重复消息数量之间的权衡&lt;/h5&gt;

&lt;p&gt;可以在一个循环里多次提交偏移量，也可以在多个循环只提交一次偏移量。&lt;/p&gt;

&lt;h5 id=&quot;注意再均衡&quot;&gt;注意再均衡&lt;/h5&gt;

&lt;p&gt;==注意要在再均衡发生前提交偏移量。==&lt;/p&gt;

&lt;h5 id=&quot;消费者重试&quot;&gt;消费者重试&lt;/h5&gt;

&lt;p&gt;在不影响轮询读取的情况下，对&lt;strong&gt;处理&lt;/strong&gt;失败的消息进行重试。&lt;/p&gt;

&lt;p&gt;如，记录#30处理失败，#31处理成功，此时需要在不丢弃#30和不影响轮询的状态下对#30进行重试。&lt;/p&gt;

&lt;p&gt;方法一：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;提交最后一个处理成功的偏移量，把处理失败的消息保存到缓冲区。&lt;/li&gt;
  &lt;li&gt;调用&lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaConsumer#pause(Collection&amp;lt;TopicPartition&amp;gt; partitions)&lt;/code&gt;使得轮询不再返回新数据。&lt;/li&gt;
  &lt;li&gt;尝试重新处理缓冲区中的消息，直至成功或到达重试上限。&lt;/li&gt;
  &lt;li&gt;调用&lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaConsumer#resume(Collection&amp;lt;TopicPartition&amp;gt; partitions)&lt;/code&gt;使得轮询返回新数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;KafkaConsumer:

public void pause(Collection&amp;lt;TopicPartition&amp;gt; partitions)

暂停从给定分区中获取数据，新的poll调用不会给消费者返回任何数据。

public void resume(Collection&amp;lt;TopicPartition&amp;gt; partitions)

从暂停中恢复。

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;==使用&lt;code class=&quot;highlighter-rouge&quot;&gt;pause&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;resume&lt;/code&gt;方法是因为，不能跳出poll循环，也不能长时间阻塞轮询，会造成长时间没有发出心跳，Kafka broker会认为消费者宕机，造成再均衡。==&lt;/p&gt;

&lt;p&gt;方法二：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;把错误写入一个独立的主题。&lt;/li&gt;
  &lt;li&gt;建立一个独立的消费者群组专门负责从错误主题上读取需要重试的数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;长时间处理&quot;&gt;长时间处理&lt;/h5&gt;

&lt;p&gt;暂停轮询的时间不能超过几秒，否则客户端和broker的心跳将断开。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用线程池处理需要长时间处理的数据。&lt;/li&gt;
  &lt;li&gt;调用pause()，保持轮询，等待工作线程完成处理。&lt;/li&gt;
  &lt;li&gt;调用resume()，继续获取数据。&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;仅一次处理&quot;&gt;仅一次处理&lt;/h5&gt;

&lt;p&gt;消费者如果要支持仅一次处理语义(及每个消息只被写到外部系统一次，不处理重复消息)。&lt;/p&gt;

&lt;p&gt;最简单的办法是把结果写到一个支持唯一键的系统里，如键值存储引擎、关系型数据库、ElasticSearch或其他数据引擎，可以在消息里直接包含一个唯一的键，也可以使用主题 + 分区 + 偏移量的组合创建唯一键。&lt;/p&gt;</content><author><name>Cheng Dongping</name></author><summary type="html">可靠性保证</summary></entry><entry><title type="html">Kafka成员管理及消息管理机制</title><link href="http://localhost:4000/2019/04/22/kafka-broker-manager-and-data-manager/" rel="alternate" type="text/html" title="Kafka成员管理及消息管理机制" /><published>2019-04-22T00:00:00+08:00</published><updated>2019-04-22T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/22/kafka-broker-manager-and-data-manager</id><content type="html" xml:base="http://localhost:4000/2019/04/22/kafka-broker-manager-and-data-manager/">&lt;h2 id=&quot;集群成员关系&quot;&gt;集群成员关系&lt;/h2&gt;

&lt;p&gt;Kafka使用ZooKeeper维护集群成员的信息，每个broker有唯一的ID，并在启动时&lt;strong&gt;创建临时节点&lt;/strong&gt;把自己的ID注册到ZooKeeper /brokers/ids路径。&lt;/p&gt;

&lt;p&gt;Kafka组件订阅ZooKeeper的/brokers/ids路径，可以获得broker创建或宕机的通知。&lt;/p&gt;

&lt;p&gt;关闭broker时，它的ID从ZooKeeper上删除，但是继续存在于其他数据结构中(如：主题的副本列表)，完全关闭一个broker后，如果使用相同的ID启动另一个全新的broker，它会立即加入集群，并拥有与旧broker相同的分区和主题。&lt;/p&gt;

&lt;h2 id=&quot;控制器controller&quot;&gt;控制器(Controller)&lt;/h2&gt;

&lt;p&gt;控制器是一个broker，除了一般broker的功能外，还负责==分区首领的选举==。&lt;/p&gt;

&lt;p&gt;集群里的broker通过在Zookeeper创建临时节点/controller竞争成为控制器。其他broker创建失败后会向/controller节点注册watch对象。&lt;/p&gt;

&lt;p&gt;当前控制器宕机后，其他broker会收到watch消息，并尝试创建/controller竞争称为新的控制器。&lt;/p&gt;

&lt;p&gt;每个新选出的控制器通过Zookeeper的条件递增操作获得一个新的controller epoch, 其他broker在知道当前controller epoch后，会忽略之前控制器发出的包含较旧epoch的消息。&lt;/p&gt;

&lt;h2 id=&quot;复制&quot;&gt;复制&lt;/h2&gt;

&lt;p&gt;Kafka是&lt;strong&gt;一个分布式的、可分区的、可复制的提交日志服务&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;Kafka使用主题来组织数据，每个主题分为若干个分区，每个分区有多个副本。&lt;/p&gt;

&lt;p&gt;==副本保存在broker上，每个broker可以保存属于不同主题和分区的多个副本。==&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;首领副本&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;每个分区只有一个首领副本，所有生产者请求和消费者请求都经过该副本。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;跟随者副本&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;除首领副本外都是跟随者副本。跟随者副本不处理来自客户端的请求，唯一的任务就是从首领那里复制消息，保持与首领一致的状态。如果首领副本所在的broker崩溃，其中的一个跟随者将成为新首领副本。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;跟随者副本—同步的副本&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;跟随者向首领发送和消费者一样的，获取数据的请求，请求包含有序的偏移量。只有收到前一个偏移量请求的回复后，才会继续请求下一个偏移量的请求。&lt;/p&gt;

&lt;p&gt;==通过查看每个跟随者请求的偏移量，首领就会知道每个跟随者复制的进度。==如果跟随者在10s内没有请求任何消息、或者虽然在请求消息，但在10s内没有请求最新的数据，就被认为是不同步的。&lt;/p&gt;

&lt;p&gt;持续请求得到最新消息的副本被称为&lt;strong&gt;同步的副本&lt;/strong&gt;，==只有同步的副本才能被选为新首领。==&lt;/p&gt;

&lt;h3 id=&quot;分区首领副本的选举&quot;&gt;分区首领副本的选举&lt;/h3&gt;

&lt;p&gt;Kafka在ZooKeeper上为每个Topic维护一个所有==同步副本的集合==，称为ISR(In-Sync Replica)。&lt;/p&gt;

&lt;p&gt;当Leader分区不可用时，控制器(Controller)broker直接从ISR列表中取出第一个broker作为新的首领，如果不行则依次类推。&lt;/p&gt;

&lt;h2 id=&quot;处理请求&quot;&gt;处理请求&lt;/h2&gt;

&lt;p&gt;Kafka提供了一个二进制协议(基于TCP)，指定了请求消息的格式以及broker如何对请求做出响应。客户端发起连接并发送请求，broker按请求到达的顺序处理请求并做出响应。&lt;/p&gt;

&lt;p&gt;标准消息头：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Request type : API key&lt;/li&gt;
  &lt;li&gt;Request version : broker可以处理不同版本的客户端请求，根据客户端版本做出不同响应。&lt;/li&gt;
  &lt;li&gt;Correlation ID : 标识请求消息。&lt;/li&gt;
  &lt;li&gt;Client ID : 表示发送请求的客户端。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;broker请求处理流程：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;brokerAcceptor线程监听端口，创建连接并交给Processor线程。&lt;/li&gt;
  &lt;li&gt;Processor线程将客户端请求放入请求队列、从响应队列获取响应消息发给客户端。
&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_broker_process_line.png&quot; alt=&quot;image&quot; /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;常见的请求类型：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;生产请求&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;生产者向broker发送要写入的消息。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;获取请求&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;消费者&lt;strong&gt;和跟随者&lt;/strong&gt;从broker读取消息。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;元数据请求&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;==生产请求和获取请求都必须发送给分区的首领副本==，如果broker收到一个针对特定分区的请求，而该分区的首领在另一个broker上，那么发送请求的客户端会收到一个 ==“非分区首领”== 错误。因此，客户端需要利用&lt;strong&gt;元数据请求&lt;/strong&gt;知道生产和获取请求的目标broker。&lt;/p&gt;

&lt;p&gt;客户端向服务器请求感兴趣的主题列表信息，服务端的响应消息里指明了主题包含的分区、每个分区有哪些副本、哪个副本是首领副本，副本所在的broker。&lt;/p&gt;

&lt;p&gt;元数据请求可以发送给任意一个broker，因为所有broker都缓存了所有主题的元数据。&lt;/p&gt;

&lt;p&gt;客户端会定期发送(metadta.max.age.ms)元数据请求刷新主题分区信息，并将这些元数据缓存在本地。
&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_client_metadata_request.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;生产请求&quot;&gt;生产请求&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;请求验证
broker收到生产请求，对请求做验证：
    &lt;ul&gt;
      &lt;li&gt;发送数据的用户是否有主题写入权限？&lt;/li&gt;
      &lt;li&gt;请求包含的acks值是否有效？(0, 1, all) ?&lt;/li&gt;
      &lt;li&gt;如果acks = all, 是否有足够多的同步副本保证消息已经被安全写入
如果此时同步副本数目小于配置，broker可以拒绝处理新消息。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;消息写入&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;验证后，消息将被写入本地磁盘(文件系统缓存)，并不保证何时刷新到磁盘上，Kafka不会一直等待数据被写到磁盘上，它依赖复制功能来保证消息的持久性。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;检查acks参数并返回&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;==如果acks=0或1, broker立即返回响应，如果acks=all，请求将被加入缓冲区，直到首领发现所有跟随者副本都复制了消息，才向客户端返回响应。==&lt;/p&gt;

&lt;h3 id=&quot;获取请求&quot;&gt;获取请求&lt;/h3&gt;

&lt;p&gt;客户端向broker请求主题分区里特定偏移量的消息：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;把
主题Test，分区0，偏移量从53开始，的消息
以及
主题Test，分区3，偏移量从64开始，的消息
发给我
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;客户端可以指定broker最多从一个分区里返回的数据上限。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果没有这个限制，broker返回大量数据有可能耗尽客户端的内存。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;客户端也可以指定broker返回数据的下限。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;即broker将等到有足够的数据量时，才返回给客户端。同时，客户端可以定义一个超时时间，当等到超时时间到达时，即使没有足够的数据量，broker也将返回。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;broker检查请求是否有效。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如，指定的偏移量在分区上是否存在，如果检查失败返回错误。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;broker向客户端发送数据。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;==Kafka使用&lt;strong&gt;零复制&lt;/strong&gt;技术向客户端发送消息==，直接把消息从文件(文件系统缓存)中发送到网络通道，不经过中间缓冲区。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;大部分客户端只能读取已经被写入所有&lt;strong&gt;同步副本&lt;/strong&gt;的数据。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;还没有足够多副本复制的消息被认为是不安全的，如果首领发生崩溃，这些消息可能丢失。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_consumer_read_leader.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果broker间的消息复制变慢，那么消息到达消费者的时间也会变长。&lt;/p&gt;

&lt;h2 id=&quot;消息存储&quot;&gt;消息存储&lt;/h2&gt;

&lt;p&gt;Kafka的基本存储单位是分区(Partition)。&lt;/p&gt;

&lt;h3 id=&quot;分区分配&quot;&gt;分区分配&lt;/h3&gt;

&lt;p&gt;==在创建主题时，Kafka首先会决定如何在broker间分配分区。==&lt;/p&gt;

&lt;p&gt;分配目标：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;在broker间平均地分布分区副本。&lt;/li&gt;
  &lt;li&gt;确保分区的不同副本分布在不同的broker上。&lt;/li&gt;
  &lt;li&gt;如果为broker指定了机架信息(或机房信息)，尽可能把每个分区的副本非配到不同机架的broker上。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;分配过程：&lt;/p&gt;

&lt;p&gt;假设有6个broker，创建包含10个分区的主题，复制系数为3，也就是有30个分区副本。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;从随机的broker开始，使用轮询的方式分配&lt;strong&gt;首领分区&lt;/strong&gt;。
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;随机选中broker 4，则首领0分配在broker 4, 首领1分配在broker 5， 首领2分配在broker0 (broker为0-5)...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;从分区首领开始，依次分配跟随者副本。
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;如首领0分配在broker4，则跟随者0在broker 5，跟随者1在broker0...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果配置了机架信息，就不是轮询broker ID，而是轮询机架ID。&lt;/p&gt;

&lt;p&gt;为分区副本分配broker目录：&lt;/p&gt;

&lt;p&gt;计算每个目录里的分区数量，新的分区总是被添加到分区数量最小的那个目录里。&lt;/p&gt;

&lt;h3 id=&quot;消息文件&quot;&gt;消息文件&lt;/h3&gt;

&lt;h5 id=&quot;文件管理&quot;&gt;文件管理&lt;/h5&gt;

&lt;p&gt;Kafka管理员能为每个主题配置数据保留期限，规定数据被删除之前可以保留多长时间，或者保留的最大数据量大小。&lt;/p&gt;

&lt;p&gt;==分区(Partition)被分成若干个片段(Segment)，默认为1G，达到片段上线，就关闭当前文件并打开一个新文件。==&lt;/p&gt;

&lt;p&gt;当前正在写入的片段文件叫做&lt;strong&gt;活跃片段&lt;/strong&gt;，活跃片段永远不会被删除。&lt;/p&gt;

&lt;h5 id=&quot;文件格式&quot;&gt;文件格式&lt;/h5&gt;

&lt;p&gt;==Kafka保存在文件的消息格式与生产者发送以及发送给消费者的格式一致。==&lt;/p&gt;

&lt;p&gt;==因为使用了相同的消息格式进行磁盘存储和网络传输，Kafka可以使用&lt;strong&gt;零复制&lt;/strong&gt;技术，同时避免在broker上对生产者压缩过的消息进行解压和再压缩。==&lt;/p&gt;

&lt;p&gt;如果生产者发送的是压缩过的消息，那么同一个批次的消息会被压缩在一起，被当做“包装消息”发送，broker将直接记录压缩消息，然后再整个批次发送给消费者。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_message_store_format.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;索引&quot;&gt;索引&lt;/h5&gt;

&lt;p&gt;Kafka broker需要迅速定位消费者要读取的偏移量位置，因此Kafka为&lt;strong&gt;每个分区&lt;/strong&gt;维护了一个索引。把偏移量映射到片段文件和偏移量在文件里的位置。&lt;/p&gt;

&lt;p&gt;索引也被分成片段，在删除消息时，也可以删除相应的索引。索引由Kafka读取消时自动生成，因此如果损坏或删除，Kafka都会自动重新生成。&lt;/p&gt;

&lt;h3 id=&quot;消息清理&quot;&gt;消息清理&lt;/h3&gt;

&lt;p&gt;清理策略：
&lt;code class=&quot;highlighter-rouge&quot;&gt;log.cleanup.policy&lt;/code&gt;=&lt;strong&gt;delete&lt;/strong&gt; / &lt;strong&gt;compact&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;delete 策略&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;根据设置的时间保留数据，把超时的旧数据删除。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;compact 策略&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;为每个键保留最新的值，删除旧值。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;无论是delete策略还是compact策略都不会清理当前活跃片段。&lt;/strong&gt;&lt;/p&gt;

&lt;h5 id=&quot;清理工作原理&quot;&gt;清理工作原理&lt;/h5&gt;

&lt;p&gt;每个broker启动一个清理管理器线程和多个(log.cleaner.threads)清理线程, 清理线程每隔一定时间(log.retention.check.interval.ms)检查是否有日志需要清理， 清理线程每次选择dirtyRatio较高的分区进行清理。&lt;/p&gt;

&lt;h5 id=&quot;delete策略&quot;&gt;delete策略&lt;/h5&gt;

&lt;ol&gt;
  &lt;li&gt;基于日志文件总大小(空间维度)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;参数：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;log.retention.bytes&lt;/code&gt; : broker级别(默认-1，未开启)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;retention.bytes&lt;/code&gt; : topic级别(默认-1，未开启)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;清理线程比较 [当前日志总大小] - [阈值] &amp;gt;= [日志段大小]，及当前所有日志段总大小是否比阈值大至少一个日志段大小，如果是，则从最老的日志段开始删除。&lt;/p&gt;

&lt;p&gt;删除的最小单位是日志段。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;基于日志分段最新修改时间(时间维度)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;检查当前日志分段文件最新修改时间，删除和当前时间差值超过设定的时间阈值的日志段。&lt;/p&gt;

&lt;p&gt;参数：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;log.retention.hours=168&lt;/li&gt;
  &lt;li&gt;log.retention.minutes=null&lt;/li&gt;
  &lt;li&gt;log.retention.ms=null&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;基于分区日志起始偏移量&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果日志段的下一个偏移量(end + 1)小于设置的起始偏移量，则删除。&lt;/p&gt;

&lt;h5 id=&quot;compact策略&quot;&gt;compact策略&lt;/h5&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;log.cleaner.enable&lt;/code&gt;=true
&lt;code class=&quot;highlighter-rouge&quot;&gt;log.cleanup.policy&lt;/code&gt;=compact&lt;/p&gt;

&lt;p&gt;每个日志片段(segment)分为两个部分：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;干净(clean)的部分：之前已经被清理过。&lt;/li&gt;
  &lt;li&gt;污浊(dirty)的部分：在上一次清理后写入的消息。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_message_clean_segments.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;==清理线程从dirtyRatio较高的分区进行清理，维护一个map，对每一个key，只保留最新值，删除就版本的数据。==&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_log_clean_compact.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;==删除完成后，偏移量可能是不连续的。==&lt;/p&gt;

&lt;p&gt;compact策略只适合对每个key的旧值不关心的特殊场景，如key是用户ID，value是用户的资料，整个消息集里只需要保存用户的最新资料。&lt;/p&gt;

&lt;p&gt;compact策略下的删除：&lt;/p&gt;

&lt;p&gt;==如果需要删除key最新的值，可以向broker发送值为null的消息(墓碑消息)，broker首先会进行常规清理，删除null之前的消息，之后，null值消息会被保存一段时间后删除。==&lt;/p&gt;</content><author><name>Cheng Dongping</name></author><summary type="html">集群成员关系</summary></entry><entry><title type="html">Kafka的消费者创建及配置</title><link href="http://localhost:4000/2019/04/20/Kafka-consumer/" rel="alternate" type="text/html" title="Kafka的消费者创建及配置" /><published>2019-04-20T00:00:00+08:00</published><updated>2019-04-20T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/20/Kafka-consumer</id><content type="html" xml:base="http://localhost:4000/2019/04/20/Kafka-consumer/">&lt;h2 id=&quot;分区分配&quot;&gt;分区分配&lt;/h2&gt;

&lt;p&gt;Kafka消费者从属于&lt;strong&gt;消费者群组&lt;/strong&gt;，一个群组里的消费者订阅的是同一个主题，每个消费者接受主题&lt;strong&gt;一部分分区&lt;/strong&gt;的消息。&lt;/p&gt;

&lt;p&gt;每个消费者群组为内部的消费者自动分配主题的分区。&lt;/p&gt;

&lt;p&gt;如果群组G1中只有一个消费者C1，C1将收到主题T1的全部4个分区。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_consumer_partition_assign_1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果群组G1中有两个消费者C1、C2，两个消费者将分别接收两个分区的消息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_consumer_partition_assign_2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果群组G1有4个消费者，每个消费者分配一个分区。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_consumer_partition_assign_3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果群组G1有大于4个消费者，只有4个分区，==&lt;strong&gt;多余的消费者将被闲置&lt;/strong&gt;==，不会接收到任何消息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_consumer_partition_assign_4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;分区再均衡&quot;&gt;分区再均衡&lt;/h2&gt;

&lt;p&gt;分区所有权从一个消费者转移到另一个消费者，称为分区再均衡。&lt;/p&gt;

&lt;p&gt;==消费者群组中消费者的加入、退出、崩溃等，都会造成分区再均衡，再均衡期间整个群组将不可用。==&lt;/p&gt;

&lt;h2 id=&quot;群组协调器groupcoordinator&quot;&gt;群组协调器(GroupCoordinator)&lt;/h2&gt;

&lt;p&gt;在 kafka-0.10 版本，Kafka 在服务端引入了组协调器(GroupCoordinator)，每个Kafka Server启动时都会创建一个GroupCoordinator实例，用于管理部分消费者组和该消费者组下的每个消费者的消费偏移量。&lt;/p&gt;

&lt;p&gt;同时在客户端引入了消费者协调器(ConsumerCoordinator)，每个消费者都会实例化一个ConsumerCoordinator，只是负责与该消费者对应的broker上的GroupCoordinator进行通信。&lt;/p&gt;

&lt;p&gt;消费者通过向broker GroupCoordinator发送心跳维持它们和群组的从属关系以及它们对分区的所有权关系。&lt;/p&gt;

&lt;p&gt;如果消费者心跳过期，群组协调器认为它已经死亡，就会触发一次再均衡。&lt;/p&gt;

&lt;h2 id=&quot;分区分配的过程&quot;&gt;分区分配的过程&lt;/h2&gt;

&lt;p&gt;消费者要加入群组时，它会向&lt;strong&gt;群组协调器&lt;/strong&gt;发送一个JoinGroup请求，第一个加入群组的消费者将成为“群主”。&lt;/p&gt;

&lt;p&gt;==群主从协调器那里获得群组的成员列表，并且负责给每一个消费者分配分区。==&lt;/p&gt;

&lt;p&gt;分配完毕后，群组把分配情况列表发给群组协调器，协调器再发送给所有消费者。&lt;/p&gt;

&lt;p&gt;每个消费者只能看到自己的分配信息，只有群主知道群组里所有消费者的分配信息。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_consumer_join_group.jpeg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;创建kafka消费者&quot;&gt;创建Kafka消费者&lt;/h2&gt;

&lt;h3 id=&quot;对象属性&quot;&gt;对象属性&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;bootstrap.servers&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;配置Kafka broker位置&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;key.deserializer&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;value.deserializer&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;group.id&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Properties props = new Properties();
pros.put(&quot;bootstrap.servers&quot;, &quot;broker1:9092,broker2:9092&quot;);
props.put(&quot;group.id&quot;,&quot;CountryCounter&quot;);
props.put(&quot;key.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);
props.put(&quot;value.deserializer&quot;, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;);

KafkaConsumer&amp;lt;String, String&amp;gt; consumer = new KafkaConsumer&amp;lt;String, String&amp;gt;(props);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;订阅主题&quot;&gt;订阅主题&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;consumer.subscribe(Collections.singletionList(&quot;customerCountries&quot;));
consumer.subscribe(&quot;test.*&quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;轮询消息&quot;&gt;轮询消息&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;try {
    while(true) { // 无线循环轮询
        ConsumerRecords&amp;lt;String, String&amp;gt; records = consumer.poll(100); // 读取一批数据。
        for(ConsumerRecord&amp;lt;String, String&amp;gt; record : records) {
            log.info(
                &quot;topic={}, partition = {}, offset = {}, customer = {}, contry = {}&quot;, 
                record.topic(), record.partition(), record.offset(), record.key(), record.value()
            );
        }
    }
} finally {
    consumer.close(); // 推出前关闭消费者
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;==消费者必须持续调用&lt;code class=&quot;highlighter-rouge&quot;&gt;poll&lt;/code&gt;方法进行轮询，否则会被认为已经死亡，分区会被移交给群组里的其他消费者。==&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;poll()&lt;/code&gt;方法参数为超时时间，返回值为一个记录列表，包含主题、分区、偏移量、键值对信息。&lt;/li&gt;
  &lt;li&gt;第一次查找群组协调器(GroupCoordinator)、加入群组、接收分区、接收分区再均衡、发送心跳包都是在&lt;code class=&quot;highlighter-rouge&quot;&gt;poll()&lt;/code&gt;方法中完成的。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;close()&lt;/code&gt;方法关闭消费者。关闭网络连接和socket，并立即触发一次&lt;strong&gt;分区再均衡&lt;/strong&gt;，而不是等待群组协调器发现它不再发送心跳并认定它已经死亡。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;kafka消费者配置&quot;&gt;Kafka消费者配置&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;client.id&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;表示消费者客户端&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;session.timeout.ms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;指定消费者被认为死亡前与服务器断开连接的时间，默认为3s。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;hearbeat.interval.ms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;指定poll()方法向协调器发送心跳的时间间隔。&lt;/p&gt;

&lt;p&gt;必须设置比session.timeout.ms小，通常为session.timeout.ms的三分之一。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;fetch.min.bytes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;指定消费者从服务器获取记录的最小字节数。&lt;/p&gt;

&lt;p&gt;消费者向broker发送poll请求时，如果可用的数据量小于fetch.min.bytes，将等待。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;fetch.max.wait.ms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设定2中可用数据量小于fetch.min.bytes时最大等待时间，默认是500ms。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;max.partition.fetch.bytes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设定服务器从每个分区里返回给消费者的最大字节数，默认为1MB。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;max.poll.records&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;控制消费者单词调用poll方法能够返回的最大记录数量。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;receive.buffer.bytes和send.buffer.bytes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设置socket在读写数据时用到的TCP缓冲区大小，-1为使用系统默认值。&lt;/p&gt;

&lt;p&gt;如果生产者或消费者与broker不在同一个数据中心，可以适当增大该值。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;auto.offset.reset&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;指定消费者在读取一个没有偏移量的分区或者偏移量无效的情况(由于消费者长时间失效，包含偏移量的记录被删除)下如何处理。&lt;/p&gt;

&lt;p&gt;latest：即从目前的最新记录开始读取。默认为latest。&lt;/p&gt;

&lt;p&gt;earliest：从分区起始位置读取。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;enable.auto.commit&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;指定消费者是否自动提交偏移量。默认为true。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;auto.commit.interval.ms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;自动提交偏移量的频率。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;partition.assiginment.strategy&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设置消费者分区分配策略，值为Range或RoundRobin。&lt;/p&gt;

&lt;h3 id=&quot;消费者分区分配策略&quot;&gt;消费者分区分配策略&lt;/h3&gt;

&lt;h4 id=&quot;range-strategy&quot;&gt;Range Strategy&lt;/h4&gt;

&lt;p&gt;Range策略对 ==&lt;strong&gt;每个主题&lt;/strong&gt;== 中的分区为消费者平均分配。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;对同一个主题的所有分区按照序号排序。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对消费者按照字母序排序。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将分区的个数除以消费者线程的总数决定每个消费者线程消费几个分区。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果除不尽，那么前面几个消费者线程将会多分配一个分区。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;p&gt;某个主题有11个分区，3个消费者C1, C2, C3.&lt;/p&gt;

&lt;p&gt;C1将消费 0,1,2,3分区。&lt;/p&gt;

&lt;p&gt;C2将消费 4,5,6,7分区。&lt;/p&gt;

&lt;p&gt;c3将消费 8,9,10分区。&lt;/p&gt;

&lt;h4 id=&quot;roundrobin-strategy&quot;&gt;RoundRobin Strategy&lt;/h4&gt;

&lt;p&gt;RoundRobin对==所有主题的所有分区==按照HashCode的值排序，并按照RoundRobin风格为每个消费者线程逐个分配。&lt;/p&gt;

&lt;p&gt;使用RoundRobin必须满足的条件：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;同一个消费者群组中所有消费者的num.streams必须相等。&lt;/li&gt;
  &lt;li&gt;所有消费者订阅相同的主题。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;例子：&lt;/p&gt;

&lt;p&gt;有T1和T2两个主题，每个主题有3个分区，分区排序为T1-1, T1-2, T2-1, T2-2, T1-3, T2-3，消费者线程排序为C1-0, C1-1, C2-0，则分配结果为：&lt;/p&gt;

&lt;p&gt;C1-0分配：T1-1, T1-3&lt;/p&gt;

&lt;p&gt;C1-1分配：T1-2, T2-3&lt;/p&gt;

&lt;p&gt;C2-0分配：T2-1&lt;/p&gt;

&lt;h2 id=&quot;提交commit和偏移量offset&quot;&gt;提交(Commit)和偏移量(Offset)&lt;/h2&gt;

&lt;p&gt;我们把更新分区当前位置的操作叫做&lt;strong&gt;提交&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;消费者通过群组协调器(Group Coordinator)往Kafka中一个叫做&lt;code class=&quot;highlighter-rouge&quot;&gt;_consumer_offset&lt;/code&gt;的特殊主题发送消息，消息里包含每个分区的偏移量。&lt;/p&gt;

&lt;p&gt;当发生再均衡时，每个消费者可能分配到新的分区，为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;提交的偏移量 小于 客户端处理的最后一个消息的偏移量，消息会被重复处理。&lt;/li&gt;
  &lt;li&gt;提交的偏移量 大于 客户端处理的最后一个消息的偏移量，消息会被丢失。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此，提交偏移量的方式非常重要。&lt;/p&gt;

&lt;h3 id=&quot;自动提交偏移量auto-commit-offset&quot;&gt;自动提交偏移量(Auto Commit Offset)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;设定enable.auto.commit = true。&lt;/li&gt;
  &lt;li&gt;auto.commit.interval.ms = 5s，设置自动提交的时间间隔，单位为秒，默认为5秒。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;自动提交虽然方便，当时无法进行精确控制，容易造成重复处理和丢失的情况。&lt;/p&gt;

&lt;h3 id=&quot;同步提交当前偏移量commitsync&quot;&gt;同步提交当前偏移量(CommitSync)&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;commitSync()&lt;/code&gt;方法：&lt;/p&gt;

&lt;p&gt;提交由poll()方法返回的最新偏移量，提交成功后马上返回，提交失败抛出异常，只要没有发生不可恢复的错误，commitSync()方法会一直尝试直至成功。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while(true) {
    ConsumerRecords&amp;lt;String, String&amp;gt; records = comsumer.poll(100);
    for(ConsumerREcord&amp;lt;String, String&amp;gt; record : records) {
        // handle the records
    }
    try {
        consumer.commitSync(); // 提交最新偏移量
    } catch (CommitFailedException e) {
        log.error(&quot;commit fail&quot;, e);
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;手动提交在处理records的循环中加入提交偏移量的请求，提交偏移量时需要阻塞等待broker返回，降低了程序的吞吐量。&lt;/p&gt;

&lt;h3 id=&quot;异步提交当前偏移量&quot;&gt;异步提交当前偏移量&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;commitAsync()&lt;/code&gt;方法，异步发送偏移量提交请求，无需阻塞等到broker返回，支持回调。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;while(true) {
    ComsumerRecords&amp;lt;String ,String&amp;gt; records = consumer.poll(100);
    for(ConsumerRecord&amp;lt;String, String&amp;gt; record : records) {
        // handle the record.
    }
    comsumer.commitAsync(new OffsetCommitCallback() { // 异步提交
        public void onComplete(Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; offsets, Exception e) {
            if (e != null) {
                log.error(&quot;Commit failed for offsets {}&quot;, offsets, e);
            }
        }
    });
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;==异步提交在失败后不会主动重试，因为此时可能有一个更大的偏移量已经完成提交。==&lt;/p&gt;

&lt;h3 id=&quot;同步和异步组合提交&quot;&gt;同步和异步组合提交&lt;/h3&gt;

&lt;p&gt;如果提交发生在关闭消费者时，或是再均衡时的最后一次提交，必须要保证提交成功，此时无法使用异步提交。&lt;/p&gt;

&lt;p&gt;因此，通常会组合使用同步提交和异步提交，==对于正常处理循环中使用异步提交、在关闭和再均衡前使用同步提交。==&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;try {
    while (true) {
        ConsumerRecords&amp;lt;String ,String&amp;gt; records = consumer.poll(100);
        for(ConsumerRecord&amp;lt;String ,String&amp;gt; record : records) {
            // handle the records
        }
        consumer.commitAsync(); // 异步提交
    }
} catch (Exception e) {
    log.error(&quot;Exception&quot;, e);
} finally {
    try {
        consumer.commitSync(); // 在关闭前尝试同步提交。
    } finally {
        consumer.close();
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;提交给定的偏移量&quot;&gt;提交给定的偏移量&lt;/h3&gt;

&lt;p&gt;如果poll()方法返回一大批数据，需要处理很长时间，希望在处理每个数据时马上记录当前处理数据的偏移量，而不是处理完这批数据后再记录整批数据的偏移量。&lt;/p&gt;

&lt;p&gt;commitSync()和commitAsync()允许添加&lt;code class=&quot;highlighter-rouge&quot;&gt;Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; map&lt;/code&gt;作为参数，将分区的偏移量设置为给定的value值。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private Map&amp;lt;TopicPartion, OffsetAndMetadata&amp;gt; currentOffsets = new HashMap&amp;lt;&amp;gt;();

// 再均衡监听器，subscribe时注册，在发生再均衡时被回调。
private class HandleRebalance implements ConsumerRebalanceListener {
    
    // 在重新分配分区之后 和 消费者开始读取消息之前被调用。
    public void onPartitionsAssigned(Collection&amp;lt;TopicPartition&amp;gt; partitions) {}
    
    // 在再均衡开始之前和消费者停止读取消息之后被调用。
    public void onPartitionsRevoked(Collection&amp;lt;TopicPartion&amp;gt; partitions) {
        log.info(&quot;Lost partitions in rebalance, Committing current offsets:&quot; + currentOffsets);
        consumer.commitSync(currentOffsets);
    }
}

int count = 0;

try {
    consumer.subscribe(topics, new HandleRebalance());
    while(true) {
        ConsumerRecords&amp;lt;String ,String&amp;gt; records = consumer.poll(100);
        for(ConsumerRecord&amp;lt;String ,String&amp;gt; record : records) {
            // handle the records
            
            currentOffsets.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1, &quot;no metadata&quot;));
            if(count % 1000 == 0) { // 每处理1000个records提交一次偏移量。
                consumer.commitAsync(currentOffsets, null);
                currentOffsets.clear();
            }
            count++;
            
        }
    }
} catch(WakeupException e) {
    
} catch (Exception e) {
    log.error(&quot;Unexcepted error&quot;, e);
} finally{
    try {
        consumer.commitSync(currentOffsets);
    } finally {
        consumer.close();
    }
}


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;从特定偏移量开始读取&quot;&gt;从特定偏移量开始读取&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;KafkaConsumer:

public void seek(TopicPartition partition, long offset); // 为指定分区设置当前偏移量。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;退出&quot;&gt;退出&lt;/h3&gt;

&lt;p&gt;==如果确定要退出循环，需要通过另一个线程调用consumer.wakeup()方法。== 如果循环运行在主线程里，可以在ShutdownHook里调用该方法。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;consumer.wakeup()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;方法是消费者唯一一个可以从其他线程里安全调用的方法，调用consumer.wakeup()可以使得主线程退出poll()并抛出WakeupException异常。&lt;/p&gt;

&lt;h2 id=&quot;独立消费者--没有群组的消费者&quot;&gt;独立消费者 – 没有群组的消费者&lt;/h2&gt;

&lt;p&gt;有时可能只需要一个消费者从一个主题的所有分区或者某个特定的分区读取数据，无需消费者群组和再均衡，只需要把主题或者分区分配给消费者，然后开始读取消息并提交偏移量。&lt;/p&gt;

&lt;p&gt;==不需要订阅主题，而是为自己直接分配分区。==&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;List&amp;lt;PartitionInfo&amp;gt; partitionInfos = null;
partitionInfos = consumer.partitionsFor(&quot;TheTopic&quot;); // 得到主题的所有分区。

if(partitionInfos != null) {
    for(PartitionInfo partition : partitionInfos) {
        partitions.add(new TopicPartition(partition.topic(), partition.partition()));
    }
    consumer.assign(partitions); // 将分区全部分配给消费者。
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Cheng Dongping</name></author><summary type="html">分区分配</summary></entry><entry><title type="html">Kafka的生产者创建及配置</title><link href="http://localhost:4000/2019/04/18/kafka-producer/" rel="alternate" type="text/html" title="Kafka的生产者创建及配置" /><published>2019-04-18T00:00:00+08:00</published><updated>2019-04-18T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/18/kafka-producer</id><content type="html" xml:base="http://localhost:4000/2019/04/18/kafka-producer/">&lt;h3 id=&quot;生产者消息发送流程&quot;&gt;生产者消息发送流程&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_producer_produce_message_processes.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;创建ProducerRecord对象，包含目标主题和要发送的内容。可以指定键或分区。&lt;/li&gt;
  &lt;li&gt;生产者把键和值对象序列化成字节数组。&lt;/li&gt;
  &lt;li&gt;如果ProducerRecord指定了分区，分区器直接返回该分区，如果没有指定分区，分区器根据ProducerRecord对象的键选择一个分区。&lt;/li&gt;
  &lt;li&gt;记录被添加到对应主题和分区的记录批次中，有一个独立的线程负责把记录批次发送到相应broker上。&lt;/li&gt;
  &lt;li&gt;服务器收到消息返回响应。如果成功写入Kafka，返回一个RecordMetaData对象，包含主题和分区信息、以及记录在分区中的偏移量。失败则返回错误。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;创建生产者&quot;&gt;创建生产者&lt;/h3&gt;

&lt;p&gt;创建Kafka生产者对象，设置3个必选属性：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;bootstrap.servers&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;指定Kafka broker地址清单，地址的格式为host:port，不需要包含所有broker地址，生产者会从给定broker里查找到其他broker的信息。&lt;/p&gt;

&lt;p&gt;建议至少提供两个broker的信息，一旦其中一个宕机，生产者仍然能够连接到集群上。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;key.serializer&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;数据需要被序列化为字节数组，key.serializer必须被设置为一个实现了&lt;code class=&quot;highlighter-rouge&quot;&gt;org.apache.kafka.common.serialization.Serializer&lt;/code&gt;接口的类。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;value.serializer&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;与key.serializer相同。&lt;/p&gt;

&lt;p&gt;发送消息举例：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;private class DemoProducerCallback implements Callback {
    @Override
    public void onCompletion(REcordMetadata recordMetadata, Exception e) {
        if (e != null) {
            e.printStackTrace();
        }
    }
}

ProducerRecord&amp;lt;String, String&amp;gt; record = enw ProducerRecord&amp;lt;&amp;gt;(&quot;TestTopic&quot;, &quot;TestKey&quot;, &quot;TestValue&quot;);
producer.send(record, new DemoProducerCallback());
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;生产者的配置&quot;&gt;生产者的配置&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;acks&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;指定必须要有多少个&lt;strong&gt;分区副本&lt;/strong&gt;收到消息，生产者才会认为消息写入是成功的。&lt;/p&gt;

&lt;p&gt;acks=0, 生产者不等待任何服务器响应，不关心发送结果，吞吐量最大。&lt;/p&gt;

&lt;p&gt;acks=1, 只要&lt;strong&gt;集群首领&lt;/strong&gt;收到消息，生产者就会收到来自服务器的成功响应。&lt;/p&gt;

&lt;p&gt;acks=all，只有当全部&lt;strong&gt;参与复制的节点&lt;/strong&gt;收到消息时，生产者才会收到来自服务器的成功响应。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;buffer.memory&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设置生产者内存缓冲区的大小。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;compression.type&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设置消息发送时压缩方式(snappy/gzip/lz4)，默认不压缩。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;retries&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;生产者从服务器收到临时性错误(如分区找不到首领)时，重试的次数。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;batch.size&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;多个消息需要被发送到同一个分区时，生产者会把他们放在同一个批次里。&lt;/p&gt;

&lt;p&gt;指定批次可以使用的最大内存大小(字节数)。当批次被填满时，批次里所有消息会被发送出去。&lt;/p&gt;

&lt;p&gt;生产者并不一定都会等到批次被填满才发送，见linger.ms。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;linger.ms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设置生产者在发送批次之前等待更多消息加入批次的时间。&lt;/p&gt;

&lt;p&gt;当&lt;strong&gt;批次填满&lt;/strong&gt;或&lt;strong&gt;linger.ms达到上限&lt;/strong&gt;时，Kafka会发送批次。&lt;/p&gt;

&lt;p&gt;linger.ms默认为0，即不等待，就算批次里只有一个消息也立即发送。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;client.id&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;生产者客户端id。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;max.in.flight.requests.per.connection&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;指定了生产者在收到服务器响应之前可以发送多少个消息。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;timtout.ms、request.timeout.ms、metadata.fetch.timtout.ms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;指定了生产者等待服务器返回响应的时间。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;max.block.ms&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设置在调用send()方法或partitionsFor()方法时生产者的阻塞时间。当生产者发现缓冲区已满，方法会阻塞，阻塞达到max.block.ms时，生产者会抛出异常。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;max.request.size&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;控制生产者发送的请求大小。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;receive.buffer.bytes 和 send.buffer.bytes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;设置TCP socket接收和发送数据包的缓冲区大小，-1为使用操作系统默认值。&lt;/p&gt;

&lt;h3 id=&quot;生产者消息的顺序&quot;&gt;生产者消息的顺序&lt;/h3&gt;

&lt;p&gt;Kafka可以保证&lt;strong&gt;同一个分区&lt;/strong&gt;里&lt;strong&gt;成功发送的&lt;/strong&gt;消息是有序的。即如果生产者按照一定的顺序成功发送消息，broker就会按照这个顺序把他们写入分区，消费者也会按照同样的顺序读取它们。&lt;/p&gt;

&lt;p&gt;如果retries大于零，同时max.in.flight.requests.per.connection设为比1大的数，那么，如果前一个批次消息写入失败，后一个批次写入成功，接着broker重试写入第一个批次，如果重试成功，则两个批次顺序倒转。&lt;/p&gt;

&lt;p&gt;==所以，如果对消息顺序性有严格要求，可以把max.in.flight.requests.per.connection设为1。==&lt;/p&gt;

&lt;h3 id=&quot;键的分区&quot;&gt;键的分区&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;拥有相同键的消息将被写到同一个分区。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;==如果键值为null，并且使用了默认的分区器，分区器将使用轮询(Round Robin)算法将消息均衡地分布到各个分区上。==&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;如果键不为空，并且使用了==默认的分区器，Kafka会对键进行散列==，根据散列值将消息映射到特定的分区上。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;==Kafka使用自己的散列算法，只有在不改变主题分区数量的情况下，键与分区之前的映射才能保持不变。==&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;可以实现Partitioner接口，实现自己定义的分区策略，将特定key映射到特定分区上。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Cheng Dongping</name></author><summary type="html">生产者消息发送流程</summary></entry><entry><title type="html">Kafka的基本概念</title><link href="http://localhost:4000/2019/04/15/kafka-intro/" rel="alternate" type="text/html" title="Kafka的基本概念" /><published>2019-04-15T00:00:00+08:00</published><updated>2019-04-15T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/15/kafka-intro</id><content type="html" xml:base="http://localhost:4000/2019/04/15/kafka-intro/">&lt;h3 id=&quot;kafka登场&quot;&gt;Kafka登场&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;消息(Message)和批次(Batch)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;消息：Kafka的数据单元。&lt;/p&gt;

&lt;p&gt;键：一个字节数组，是消息的一个可选的元数据。&lt;/p&gt;

&lt;p&gt;批次：一组消息，属于同一个主题，消息被分批写入Kafka。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;模式(Schema)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Kafka默认使用Apache Avro作为序列化框架。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;主题(Topic)和分区(Partition)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Kafka的消息通过主题进行分类。==主题被分为若干个分区，一个分区就是一个提交日志。==&lt;/p&gt;

&lt;p&gt;消息以追加的方式写入分区，以先入先出的顺序读取，由于一个主题一般包含几个分区，因此无法在整个主题范围内保证消息的顺序，但可以保证消息在单个分区内的顺序。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_topic_patition_message_write_sample.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;生产者(Producer)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;生产者在默认情况下把消息均衡地分布到主题的所有分区上，并不关心特定消息会被写入到哪个分区。&lt;/p&gt;

&lt;p&gt;在某些情况下，生产者会把消息直接写到指定的分区，这通常是通过消息键(Key)和分区器实现的，分区器为键生成一个散列值，并将其映射到指定的分区上。这样可以保证包含同一个键的消息会被写到同一个分区上。&lt;/p&gt;

&lt;p&gt;生产者也可以使用自定义的分区器，根据不同的业务规则将消息映射到分区。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;消费者(Consumer)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;消费者订阅一个或多个&lt;strong&gt;主题&lt;/strong&gt;，并按照消息生成的顺序读取它们。消费者通过检查消息的&lt;strong&gt;偏移量&lt;/strong&gt;来区分已经读取过的消息。&lt;/p&gt;

&lt;p&gt;偏移量：是一个不断递增的整数值，在创建消息时，Kafka会把它添加到消息里。在给定的分区里，每个消息的偏移量都是唯一的。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;消费者群组(Consumer Group)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;消费者是消费者群组的一部分，也就是说，会有一个或多个消费者共同读取一个主题。&lt;/p&gt;

&lt;p&gt;群组保证 &lt;strong&gt;每个分区只能被一个消费者使用。&lt;/strong&gt; 消费者与分区之间的映射通常被称为消费者对分区的所有权关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_consumer_group.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;broker&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;一个独立的Kafka服务器被称为broker。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;集群(Cluster)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;broker是集群的组成部分，每个集群都由一个broker同时充当了&lt;strong&gt;集群控制器&lt;/strong&gt;的角色。&lt;/p&gt;

&lt;p&gt;在集群中，一个分区从属于一个broker，该broker被称为分区的&lt;strong&gt;首领&lt;/strong&gt;，一个分区可以分配给多个broker，这个时候会发生&lt;strong&gt;分区复制&lt;/strong&gt;，这种复制机制为分区提供了消息冗余，如果有一个broker失效，其他broker可以接管领导权。不过，相关的消费者和生产者都要重新连接到新的首领。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/kafka_partition_copy.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;</content><author><name>Cheng Dongping</name></author><summary type="html">Kafka登场</summary></entry><entry><title type="html">ZooKeeper数据存储与数据同步机制</title><link href="http://localhost:4000/2019/04/13/zookeeper-data-storage-and-data-sync/" rel="alternate" type="text/html" title="ZooKeeper数据存储与数据同步机制" /><published>2019-04-13T00:00:00+08:00</published><updated>2019-04-13T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/13/zookeeper-data-storage-and-data-sync</id><content type="html" xml:base="http://localhost:4000/2019/04/13/zookeeper-data-storage-and-data-sync/">&lt;p&gt;ZooKeeper中，数据存储分为两部分，内存数据(ZKDatabase)与磁盘数据(事务日志 + 事务快照)。&lt;/p&gt;

&lt;h2 id=&quot;zkdatabase&quot;&gt;ZKDatabase&lt;/h2&gt;

&lt;p&gt;ZooKeeper的数据模型是一棵树。&lt;/p&gt;

&lt;p&gt;而从使用角度看，ZooKeeper就像一个内存数据库一样，在内存数据库中，存储了整棵树的内容，包括所有的节点路径、节点数据以及ACL信息等。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ZKDatabase&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ZKDatabase是ZooKeeper的内存数据库，负责管理ZooKeeper的所有会话、DataTree存储和事务日志。&lt;/p&gt;

&lt;p&gt;==ZKDatabase会定时向磁盘dump快照数据，同时在ZooKeeper服务器启动的时候，会通过磁盘上的事务日志和快照数据文件恢复成一个完整的内存数据库。==&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;DateTree&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;DateTree是ZooKeeper内存数据存储的核心。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DataTree:
- nodes: ConcurrentHashMap&amp;lt;String, DataNode&amp;gt;
- ephemerals: ConcurrentHashMap&amp;lt;Long, HashSet&amp;lt;String&amp;gt;&amp;gt;
- dataWatches: WatchManager
- childWatches: WatchManager
-----------------------------------------------------
+ convertAcls(List&amp;lt;ACL&amp;gt;): Long
+ convertLong(Long): List&amp;lt;ACL&amp;gt;
+ addDataNode(String, DataNode): void
+ createNode(String, byte, List&amp;lt;ACL&amp;gt;, long, int, long, long): String
+ deleteNode(String, long)
+ setData(String, byte, int, long, long)
+ getData(String, Stat, Watcher)
+ ......
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ConcurrentHashMap&amp;lt;String, DataNode&amp;gt; nodes&lt;/code&gt;存储所有ZooKeeper节点信息，Key为节点路径，Value为DataNode。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ConcurrentHashMap&amp;lt;Long, HashSet&amp;lt;String&amp;gt;&amp;gt; ephemerals&lt;/code&gt;存储所有临时节点的信息，便于实时访问和及时清理。Key为客户端SessionID，Value为该客户端创建的所有临时节点路径集合。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;DataNode&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;DataNode 是数据存储的最小单元，内部保存节点的数据内容(data[])、ACL列表(acl)和节点状态(stat)，同时记录父节点(parent)的引用和子节点列表(children)。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DataTree:
- parent: DataNode
- data: byte[]
- acl: Long
- stat: StatPersisted
- children: Set&amp;lt;String&amp;gt;
-----------------------
+ addChild(): boolean
+ removeChild(): boolean
+ setChildren(): void
+ getChildren(): Set&amp;lt;String&amp;gt;
+ copyStat(Stat): void
+ deserialize(InputArchive, String)
+ Serialize(OutputArchive, String)
+ ......
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;事务日志&quot;&gt;事务日志&lt;/h2&gt;

&lt;h3 id=&quot;文件存储&quot;&gt;文件存储&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;配置目录&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;事务日志文件默认存储于&lt;code class=&quot;highlighter-rouge&quot;&gt;dataDir&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;也可以为事务日志单独配置文件存储目录&lt;code class=&quot;highlighter-rouge&quot;&gt;dataLogDir&lt;/code&gt;。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;存储文件&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ZooKeeper运行一段时间后，在配置的目录中将创建子目录version-2：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{dataLogDir配置目录}/version-2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;version-2是当前ZooKeeper使用的事务日志格式版本号。&lt;/p&gt;

&lt;p&gt;version-2中生成日志文件如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_tx_file_format.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;文件名&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;==事务日志文件的文件名是一个十六进制数字，高32位为Leader选举周期(epoch)，低32为是事务ZXID。==&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;日志格式&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;日志文件是二进制格式存储，ZooKeeper提供了解码工具：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Java LogFormatter 日志文件
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;第一行：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ZooKeeper Transactional Log File with dbid 0 txnlog format version 2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;事务日志文件头信息。&lt;/p&gt;

&lt;p&gt;第二行：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;..11:07:41 session 0x144699552020000 cxid 0x0 zxid 0x300000002 createSession 3000
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;一次客户端会话创建的事务操作日志。&lt;br /&gt;
事务操作时间 + 客户端会话ID + CXID + ZXID + 操作类型 + 会话超时时间&lt;/p&gt;

&lt;p&gt;第三行：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;..11:08:40 session 0x144699552020000 cxid 0x2 zxid 0x300000003 create `/test_log,#7631,v{s{31,s{'world',anyone}}},F,2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;节点创建操作的事务操作日志。&lt;br /&gt;
事务操作时间 + 客户端会话ID + CXID + ZXID + 操作类型 + 节点路径 + 节点数据内容&lt;/p&gt;

&lt;p&gt;以后几行都类似。&lt;/p&gt;

&lt;h3 id=&quot;日志写入&quot;&gt;日志写入&lt;/h3&gt;

&lt;p&gt;事务写入事务日志的操作由&lt;code class=&quot;highlighter-rouge&quot;&gt;FileTxnLog&lt;/code&gt;的&lt;code class=&quot;highlighter-rouge&quot;&gt;append&lt;/code&gt;方法完成：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public synchronized boolean append(TxnHeader hdr, Record txn)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ol&gt;
  &lt;li&gt;确定是否有事务日志可写&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ZooKeeper第一次写入事务日志，或者上一个事务日志写满时，服务器没有和任何日志文件关联，
此时需要使用&lt;strong&gt;当前待写入事务的ZXID作为后缀创建新的事务日志文件&lt;/strong&gt;，并写入。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;确定事务日志文件是否需要扩容&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;为了避免开辟新磁盘块的开销，==ZooKeeper使用&lt;strong&gt;事务文件预分配&lt;/strong&gt;的方式。==&lt;/p&gt;

&lt;p&gt;文件初创建时，会预分配64MB磁盘块，并且当检测到当前事务文件剩余空间不足4KB时，文件大小将被增加64MB，并使用0填充被扩容的文件空间。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;zookeeper.preAllocSize&lt;/code&gt;设置预分配大小。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;写入文件&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;事务序列化、计算Checksum后，事务头、事务体和Checksum值将被写入文件流，放入streamsToFlush中。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;zookeeper.forceSync&lt;/code&gt;设置是否强制将streamsToFlush中的字节流马上写入磁盘。&lt;/p&gt;

&lt;h3 id=&quot;日志截断&quot;&gt;日志截断&lt;/h3&gt;

&lt;p&gt;在ZooKeeper中，Leader服务器上的事务ID(Zxid)必须大于或等于非Leader服务器上的事务ID(peerLastZxid)。&lt;/p&gt;

&lt;p&gt;当发现非Leader服务器上的Zxid比Leader服务器上的Zxid大时，Leader会发送TRUNC命令给该机器，进行日志截断，删除所有包含或大于peerLastZxid的事务日志文件，并重新与Leader进行同步。&lt;/p&gt;

&lt;h2 id=&quot;snapshot数据快照&quot;&gt;snapshot数据快照&lt;/h2&gt;

&lt;p&gt;数据快照用来记录ZooKeeper服务器上某一时刻的全量内存数据内容，并将其写入到指定的磁盘文件中。&lt;/p&gt;

&lt;h3 id=&quot;文件存储-1&quot;&gt;文件存储&lt;/h3&gt;

&lt;p&gt;快照数据的存储和事务日志文件类似。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;通过&lt;code class=&quot;highlighter-rouge&quot;&gt;dataDir&lt;/code&gt;属性配置文件存储位置&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;建立版本目录&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;文件名高32位为Leader选举纪元(epoch)，低32位为快照开始时最新ZXID。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;二进制存储，提供&lt;code class=&quot;highlighter-rouge&quot;&gt;SnapshotFormatter&lt;/code&gt;解码工具&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;==snapshot数据快照因为是一次全量写入，因此不需要预分配机制。==&lt;/p&gt;

&lt;h3 id=&quot;快照过程&quot;&gt;快照过程&lt;/h3&gt;

&lt;p&gt;FileSnap负责维护快照数据的接口，包括快照数据写入和读取。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;确定是否需要进行数据快照&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;==ZooKeeper每隔若干次事务日志记录后，进行一次数据快照。通过&lt;code class=&quot;highlighter-rouge&quot;&gt;snapCount&lt;/code&gt;参数进行配置。==&lt;/p&gt;

&lt;p&gt;如果当前已经记录的事务日志数量logCount满足以下“过半随机”条件时，进行一次快照：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;randRoll = random(1, snapCount / 2);
logCount &amp;gt; (snapCount / 2 + randRoll);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;snapCount&lt;/code&gt;默认为100000，那么ZooKeeper会在50000到100000次事务日志记录后进行一次快照。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;==切换事务日志文件==&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;==重新创建一个新的&lt;strong&gt;事务日志&lt;/strong&gt;==。&lt;/p&gt;

&lt;p&gt;==事务文件不能无限制增加(按64M增量)，当事务执行数目满足&lt;code class=&quot;highlighter-rouge&quot;&gt;snapCount过半随机&lt;/code&gt;时，会切换新的事务文件。==&lt;/p&gt;

&lt;p&gt;==因此快照和事务文件其实是相互影响的一体的，并不是独立的。==&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;创建数据快照异步线程&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;生成快照数据文件名&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ZooKeeper根据当前Leader纪元(epoch)及当前ZXID生成快照数据文件名。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;序列化ZKDatabase中DataTree及会话信息，生成Checksum，写入快照文件。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;内存数据初始化&quot;&gt;内存数据初始化&lt;/h2&gt;

&lt;p&gt;ZooKeeper服务器启动时，会进行数据初始化工作，将磁盘上的数据文件加载到ZooKeeper服务器内存中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_init_load_data.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;初始化FileTxnSnapLog&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;FileTxnSnapLog是ZooKeeper事务日志和快照数据访问层。包括FileTxnLog和FileSnap分别为事务日志管理器和快照数据管理器。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;初始化ZKDatabase&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;初始化DataTree，创建默认节点&lt;code class=&quot;highlighter-rouge&quot;&gt;/&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;/zookeeper&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;zookeeper/quota&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;初始化sessionsWithTimeouts会话超时时间记录器。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;创建PlayBackListener监听器&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在ZooKeeper数据恢复后期，会有一个&lt;strong&gt;事务订正&lt;/strong&gt;的过程，在这个过程中，会回调PlayBackListener监听器进行对应的数据订正。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;获取并解析快照文件&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从所有的快照文件中，按时间逆序对快照文件进行反序列化，生成DataTree对象和sessionsWithTimeouts集合，并且进行checkSum校验。&lt;/p&gt;

&lt;p&gt;只有当最新的文件不可用时，才会解析下一个，直到有一个文件通过校验，恢复完成。&lt;/p&gt;

&lt;p&gt;如果读取至第100个快照文件仍然不可用，则认为无法从磁盘中加载数据，服务启动失败。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;生成快照最新的ZXID：zxid_for_snap&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;根据4中的快照文件名低32位得到快照文件恢复数据对应的最新的ZXID: zxid_for_snap。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;解析事务日志&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;由于快照文件是依据每隔一段时间才生成，包含的数据只是近似全量数据，剩余的增量数据需要从事务日志中获&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;事务应用&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从事务日志中获取所有ZXID大于zxid_for_snap的事务，并逐个应用到DataTree和sessionsWithTimeouts中。&lt;/p&gt;

&lt;p&gt;对每个应用的事务回调PlayBackListener监听器，将事务转换成Proposal保存至提议缓存队列ZKDatabase.committedLog中，以便Follower进行快速同步。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;获取最新ZXID&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所有待提交事务被完整应用后，获取此时最大ZXID。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;校验epoch&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从最新ZXID中解析出事务处理的Leader周期epochOfZxid，同时从磁盘的currentEpoch和acceptedEpoch文件中读取上次记录的最新epoch值，进行校验。&lt;/p&gt;

&lt;h2 id=&quot;数据同步&quot;&gt;数据同步&lt;/h2&gt;

&lt;p&gt;集群完成Leader选举后，Learner会向Leader服务器进行注册，当Learner服务器向Leader完成注册后，就进入数据同步环节。&lt;/p&gt;

&lt;p&gt;数据同步过程就是Leader服务器将那些没有在Learner服务器上提交过的事务请求同步给Learner服务器。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_data_sync.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;数据同步初始化&quot;&gt;数据同步初始化&lt;/h3&gt;

&lt;p&gt;Learner向Leader注册的最后阶段，Learner向Leader发送ACKEPOCH，包含Learner的currentEpoch和lastZxid。&lt;/p&gt;

&lt;p&gt;Leader服务器从ZooKeeper内存中提取出&lt;strong&gt;提议缓存队列(committedLog)&lt;/strong&gt;，同时初始化三个ZXID值：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;committedLog: ZooKeeper会保存最近一段时间内执行的事务请求议案，个数限制默认为500个议案。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;peerLastZxid：Learner服务器的lastZxid。&lt;/li&gt;
  &lt;li&gt;minCommittedLog：Leader服务器提议缓存队列committedLog中的最小ZXID。&lt;/li&gt;
  &lt;li&gt;maxCommittedLog：Leader服务器提议缓存队列committedLog中的最大ZXID。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Leader服务器根据peerLastZxid、minCommittedLog、maxCommittedLog的值决定数据同步类型：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;差异化同步(DIFF同步)&lt;/li&gt;
  &lt;li&gt;回滚同步(TRUNC同步)&lt;/li&gt;
  &lt;li&gt;先回滚再差异化同步(TRUNC + DIFF同步)&lt;/li&gt;
  &lt;li&gt;全量同步(SNAP同步)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;差异化同步diff同步&quot;&gt;差异化同步(DIFF同步)&lt;/h3&gt;

&lt;p&gt;当 &lt;code class=&quot;highlighter-rouge&quot;&gt;minCommittedLog&lt;/code&gt; &amp;lt;= &lt;code class=&quot;highlighter-rouge&quot;&gt;peerListZxid&lt;/code&gt; &amp;lt;= &lt;code class=&quot;highlighter-rouge&quot;&gt;maxCommittedLog&lt;/code&gt;时，进行差异化同步。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_sync_diff.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Leader向Learner发送DIFF指令。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;通知Learner进入差异化数据同步阶段，Leader即将把Proposal同步给自己。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Leader针对每个Proposal，先后发送PROPOSAL内容数据包和COMMIT指令数据包&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Learner依次Proposal应用到内存数据库中。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Leader发送完差异事务数据后，立即向Learner发送NEWLEADER指令&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;NEWLEADER指令通知Learner，已经将committedLog中的Proposal都同步给Learner。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Learner向Leader反馈ACK消息&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Learner向Leader反馈完成了对committedLog中Proposal的同步。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Leader进入“过半策略”等待阶段&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Leader会和其他所有Learner服务器进行同样的数据同步流程，直到集群中由过半的Learner响应并反馈ACK消息。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;向所有已经完成数据同步的Learner发送UPTODATE指令&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;当收到过半Learner的ACK消息后，通知Learner集群中已经有过半机器完成了数据同步，已经具备对外服务的能力。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Learner再次向Leader反馈ACK。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;先回滚再差异化同步trunc--diff同步&quot;&gt;先回滚再差异化同步(TRUNC + DIFF同步)&lt;/h3&gt;

&lt;p&gt;当Leader服务器发现某个Learner包含一条自己没有的事务记录，就需要让该Learner进行事务回滚–回滚到Leader服务器上存在的，最接近peerLastZxid的ZXID。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_trunc_diff_exampe.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;在minCommittedLog &amp;lt;= peerLastZxid &amp;lt;= maxCommittedLog时，有一种特殊的情况：

1. 假设有A、B、C三台机器，此时B是Leader服务器，Leader_Epoch为5，当前已经被集群中绝大部分机器都提交的ZXID为:0x500000001和0x500000002。
2. 此时Leader正要处理ZXID: 0x500000003并且已经写入Leader本地事务日志，但是在要将该Proposal发送给其他Follower投票时Leader服务器宕机，Proposal没有被同步出去。
3. 此时ZooKeeper集群进行新一轮选举，产生的新的Leader是A，同时Leader_Epoch变更为6。
4. A和C继续提供服务，并提交了0x600000001和0x600000002两个事务。
5. 此时，服务器B再次启动，作为Follower连接至新的LeaderA，并开始同步数据。

此时，数据同步各值为：
- minCommittedLog: 0x500000001
- maxCommittedLog: 0x600000002
- peerLastZxid: 0x500000003
这种情况就需要进行TRUNC + DIFF同步，让Learner先TRUNC回滚到0x50000002，在DIFF同步至0x50000003。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;仅回滚同步trunc同步&quot;&gt;仅回滚同步(TRUNC同步)&lt;/h3&gt;

&lt;p&gt;当peerLastZxid比Leader中maxCommittedLog大时，Leader会要求Learner回滚到ZXID值为maxCommittedLog对应的事务操作。&lt;/p&gt;

&lt;h3 id=&quot;全量同步snap同步&quot;&gt;全量同步(SNAP同步)&lt;/h3&gt;

&lt;p&gt;当peerLastZxid小于minCommittedLog时，或者Leader服务器上没有提议缓存队列时，无法直接使用提议缓存队列和Learner进行数据同步。&lt;/p&gt;

&lt;p&gt;只能进行全量同步(SNAP同步)，将本机上的全量内存数据都发送给Learner。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Leader服务器向Learner发送SNAP指令。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;通知Learner即将进行全量数据同步。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;==Leader从内存数据库中&lt;strong&gt;获取到全量数据节点和会话超时时间记录器，序列化后传输给Learner&lt;/strong&gt;。==&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Learner接收到全量数据后，反序列化并载入。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Cheng Dongping</name></author><summary type="html">ZooKeeper中，数据存储分为两部分，内存数据(ZKDatabase)与磁盘数据(事务日志 + 事务快照)。 ZKDatabase ZooKeeper的数据模型是一棵树。 而从使用角度看，ZooKeeper就像一个内存数据库一样，在内存数据库中，存储了整棵树的内容，包括所有的节点路径、节点数据以及ACL信息等。 ZKDatabase</summary></entry><entry><title type="html">ZooKeeper角色、通信及请求处理</title><link href="http://localhost:4000/2019/04/12/zookeeper-server-role-and-communication/" rel="alternate" type="text/html" title="ZooKeeper角色、通信及请求处理" /><published>2019-04-12T00:00:00+08:00</published><updated>2019-04-12T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/12/zookeeper-server-role-and-communication</id><content type="html" xml:base="http://localhost:4000/2019/04/12/zookeeper-server-role-and-communication/">&lt;h2 id=&quot;服务器角色&quot;&gt;服务器角色&lt;/h2&gt;

&lt;p&gt;ZooKeeper集群中，分别有Leader、Follower和Observer三种类型的服务器角色。&lt;/p&gt;

&lt;p&gt;ZooKeeper使用责任链模式处理每一个客户端的请求。&lt;/p&gt;

&lt;h3 id=&quot;leader&quot;&gt;Leader&lt;/h3&gt;

&lt;p&gt;Leader是事务请求的唯一调度者和处理者，保证集群事务处理的顺序性。&lt;/p&gt;

&lt;p&gt;Leader是集群内部各服务器的调度者。&lt;/p&gt;

&lt;h4 id=&quot;leader的请求处理链&quot;&gt;Leader的请求处理链&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_leader_responsibility_pattern.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;PrepRequestProcessor 事务预处理处理器&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;识别当前请求是否是事务请求，并对事务请求进行预处理，如创建请求事务头、事务体、会话检查、ACL检查和版本检查等。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ProposalRequestProcessor 事务投票处理器&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ProposalRequestProcessor是Leader服务器事务处理流程的发起者。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;对于事务请求，根据请求类型创建对应的Proposal提议，并发送给所有的Follower服务器，发起一次集群内的事务投票。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将事务请求交付给SyncRequestProcessor进行事务日志的记录。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将所有请求交给CommitProcessor。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;SyncRequestProcessor 事务日志记录处理器&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将事务请求记录到事务日志文件中，同时触发ZooKeeper进行数据快照。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;AckRequestProcessor&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;负责在事务日志记录处理器完成记录后，向Proposal的投票收集器发送ACK反馈，以通知投票收集器当前服务器已经完成了对该Proposal的事务日志记录。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CommitProcessor 事务提交处理器&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;对事务请求，等待集群内针对Proposal的投票直到该Proposal可被提交。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对非事务请求，直接交付给下一级处理器ToBeCommitProcessor。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;ToBeCommitProcessor&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将被CommitProcessor处理过的可被提交的Proposal逐个交付给FinalRequestProcessor处理器。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;FinalRequestProcessor&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;最后一个处理器，进行客户端请求返回之前的收尾工作。&lt;/p&gt;

&lt;h4 id=&quot;learnerhandler&quot;&gt;LearnerHandler&lt;/h4&gt;

&lt;p&gt;Leader服务器会与每一个Follower/Observer服务器建立一个TCP长连接，同时也会为每个Follower/Observer服务器都创建一个名为LearnerHandler的实体。Leader服务器保存了所有Follower/Observer对应的LearnerHandler。&lt;/p&gt;

&lt;p&gt;LearnerHandler主要负责Follower/Observer服务器和Leader服务器之间的一系列网络通信，包括数据同步、请求转发和Proposal提议的投票等。&lt;/p&gt;

&lt;h3 id=&quot;follower&quot;&gt;Follower&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;处理客户端非事务请求，转发事务请求给Leader服务器。&lt;/li&gt;
  &lt;li&gt;参与事务请求Proposal的投票。&lt;/li&gt;
  &lt;li&gt;参与Leader选举投票。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;follower的请求处理链&quot;&gt;Follower的请求处理链&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_follower_responsibility_patterh.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;FollowerRequestProcessor&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;识别当前请求是否是事务请求，如果是事务请求则转发给Leader服务器。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CommitProcessor&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;同Leader。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SyncRequestProcessor&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;同Leader。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SendAckRequestProcessor&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;和Leader上的AckRequestProcessor想听，在SyncRequestProcessor处理器完成事务日志记录后，会向Leader服务器发送ACK消息以表明自身完成了事务日志的记录工作。&lt;/p&gt;

&lt;h3 id=&quot;observer&quot;&gt;Observer&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;和Follower一样，事务请求转发给Leader服务器进行处理。&lt;/li&gt;
  &lt;li&gt;和Follower的区别在，Observer不参与任何投票，包括Proposal投票和Leader选举投票。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Observer的请求链路和Follower也一样。&lt;/p&gt;

&lt;h2 id=&quot;服务器消息通信&quot;&gt;服务器消息通信&lt;/h2&gt;

&lt;p&gt;ZooKeeper服务器间消息类型分为：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;数据同步型&lt;/li&gt;
  &lt;li&gt;服务器初始化型&lt;/li&gt;
  &lt;li&gt;请求处理型&lt;/li&gt;
  &lt;li&gt;会话管理型&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;数据同步型&quot;&gt;数据同步型&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;消息类型&lt;/th&gt;
      &lt;th&gt;发送方&lt;/th&gt;
      &lt;th&gt;接收方&lt;/th&gt;
      &lt;th&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;DIFF&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;Leader通知Learner服务器，Leader即将与其进行DIFF方式的数据同步&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SNAP&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;Leader通知Learner服务器，Leader即将与其进行全量方式的数据同步&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;UPTODATE&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;Leader通知Learner服务器，已经完成数据同步，可以开始对外提供服务&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TRUNC&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;Leader触发Learner进行内存数据库的回滚&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;服务器初始化型&quot;&gt;服务器初始化型&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;消息类型&lt;/th&gt;
      &lt;th&gt;发送方&lt;/th&gt;
      &lt;th&gt;接收方&lt;/th&gt;
      &lt;th&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;OBSERVERINFO&lt;/td&gt;
      &lt;td&gt;Observer&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Observer启动时向Leader服务器注册自己，表明当前角色是Observer。消息中包含服务器SID和ZXID。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FOLLOWERINFO&lt;/td&gt;
      &lt;td&gt;Follower&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Follower启动时向Leader注册自己，表明当前角色是Follower，消息中包含服务器SID和ZXID。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LEADERINFO&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;Leader在接收到OBSERVERINFO和FOLLOWERINFO后回复LEARNERINFO，包含当前Leader服务器的EPOCH值。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ACKEPOCH&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Follower和Observer接收到LEADERINFO后，回复ACKEPOCH，包含最新的ZXID和EPOCH。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NEWLEADER&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;Leader在和Learner完成一个交互流程后，向Learner发送NEWLEADER消息，同时带上当前Leader服务器最新ZXID。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;请求处理型&quot;&gt;请求处理型&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;消息类型&lt;/th&gt;
      &lt;th&gt;发送方&lt;/th&gt;
      &lt;th&gt;接收方&lt;/th&gt;
      &lt;th&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;REQUEST&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Learner服务器向Leader服务器转发事务请求&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PROPOSAL&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Follower&lt;/td&gt;
      &lt;td&gt;ZAB算法核心消息，Leader服务器将事务请求以PROPOSAL消息的形式创建投票发送给所有Follower进行事务日志记录&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ACK&lt;/td&gt;
      &lt;td&gt;Follower&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Follower接收到来自Leader的PROPOSAL消息后，进行事务日志记录，完成后反馈ACK给Leader。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;COMMIT&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Follower&lt;/td&gt;
      &lt;td&gt;用于通知集群中所有的Follower服务器，可以进行事务请求的提交了。Leader服务器在接收到过半的Follower反馈的ACK消息后，生成COMMIT消息，告知所有的Follower服务器进行事务请求的提交。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;INFORM&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Observer&lt;/td&gt;
      &lt;td&gt;事务提交阶段，对Follower只需要发送COMMIT，因为之前发送的Proposal中已经包含事务内容，Follower可以从缓存中再次获取到事务请求并执行提交。&lt;br /&gt; 对Observer，因为之前没有参与事务投票，因此需要另外的INFORM消息，消息中包含事务请求的内容。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SYNC&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;通知Learner服务器已经完成了Sync操作&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;会话管理型&quot;&gt;会话管理型&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;消息类型&lt;/th&gt;
      &lt;th&gt;发送方&lt;/th&gt;
      &lt;th&gt;接收方&lt;/th&gt;
      &lt;th&gt;说明&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;PING&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;用于Leader同步Learner服务器上的&lt;strong&gt;客户端&lt;/strong&gt;心跳检测。&lt;br /&gt;ZooKeeper客户端随机和任意一个Zookeeper服务器保持连接，因此Leader服务器需要委托给Learner来保存这些客户端的心跳检测记录。&lt;br /&gt;Leader定时向Learner发送PING消息，Learner将这段时间内保持心跳检测的客户端列表，同样以PING消息的形式反馈给Leader。&lt;br /&gt;Leader服务器逐个对接收到的客户端进行会话激活。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REVALIDATE&lt;/td&gt;
      &lt;td&gt;Learner&lt;/td&gt;
      &lt;td&gt;Leader&lt;/td&gt;
      &lt;td&gt;在客户端重连时，重新连接上的新服务器会向Leader发送REVALIDATE确定会话是否已经超时，同时也激活会话。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;请求处理&quot;&gt;请求处理&lt;/h2&gt;

&lt;p&gt;ZooKeeper服务端对于会话创建的处理，大体可以分为请求接收、会话创建、预处理、事务处理、事务应用和会话响应6大环节。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_server_session_start_process.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;请求接收&quot;&gt;请求接收&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;NIOServerCnxn接收请求&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;判断是否是客户端“会话创建”请求&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;每个会话对应一个NIOServerCnxn实体，如果当前NIOServerCnxn实体未初始化，就是“会话创建”请求。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;反序列化ConnectRequest请求&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;判断是否是ReadOnly客户端&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果服务器是以ReadOnly模式启动，那么所有来自非ReadOnly客户端的请求将无法被处理。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;检查客户端ZXID&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;同一个ZooKeeper集群中，服务端的ZXID必定大于客户端的ZXID，否则服务端将不接受客户端的“会话创建”请求。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;协商sessionTimeout&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;客户端构造ZooKeeper实例时，会向服务端发送sessionTimeout参数。&lt;/p&gt;

&lt;p&gt;服务端对超时时间的限制介于2个tickTime到20个tickTime之间，如果tickTime为2000毫秒，服务端会限制客户端sessionTimeout在4秒到40秒之间。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;判断是否需要重新创建会话&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果客户端请求包含了sessionID，认为客户端正在进行会话重连，服务端只需要重新打开该会话。否则会进入下一步，为客户端创建会话。&lt;/p&gt;

&lt;h3 id=&quot;会话创建&quot;&gt;会话创建&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;为客户端生成全局唯一sessionID&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;注册会话&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;向sessionTracker维护的数据结构sessionsWithTimeout和sessionsById中插入sessionID。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;激活会话&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在ZooKeeper会话管理的桶(分桶策略)中为会话安排一个区块。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;生成会话密码&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;服务端在创建客户端会话时，会同时为客户端生成一个会话密码，连同sessionID一起发送给客户端。&lt;/p&gt;

&lt;p&gt;会话密码是会话在集群中不同机器间转移的凭证。&lt;/p&gt;

&lt;h3 id=&quot;预处理&quot;&gt;预处理&lt;/h3&gt;

&lt;p&gt;请求交给PrepRequestProcessor进行处理。&lt;/p&gt;

&lt;p&gt;对于事务请求，创建事务头和事务体。&lt;/p&gt;

&lt;h3 id=&quot;事务处理&quot;&gt;事务处理&lt;/h3&gt;

&lt;p&gt;请求交给ProposalRequestProcessor，请求的处理将会进入三个子处理流程：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sync流程&lt;/li&gt;
  &lt;li&gt;Proposal流程&lt;/li&gt;
  &lt;li&gt;Commit流程&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_leader_responsibility_pattern.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sync流程、Proposal流程和Commit流程是同时发生的。&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;sync流程&quot;&gt;Sync流程&lt;/h4&gt;

&lt;p&gt;ProposalRequestProcessor处理器，针对事务请求，使用SyncRequestProcessor处理器记录事务日志。&lt;/p&gt;

&lt;p&gt;Leader服务器和Follower服务器都有该处理器。&lt;/p&gt;

&lt;p&gt;完成事务日志记录后，Follower服务器会向Leader服务器发送ACK消息，表明自身完成了事务日志的记录，以便Leader服务器统计每个事务请求的投票情况。&lt;/p&gt;

&lt;h4 id=&quot;proposal流程&quot;&gt;Proposal流程&lt;/h4&gt;

&lt;p&gt;每一个事务请求都需要集群中过半机器投票认可才能被真正应用到ZooKeeper的内存数据库中。Proposal流程负责该投票与统计过程。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;发起投票&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果当前请求是事务请求，Leader服务器将发起一轮事务投票。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;生成提议Proposal&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将PrepRequestProcessor创建的事务头、事务体及Leader的ZXID序列化到Proposal对象中。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;广播提议&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Leader服务器以ZXID作为标识，将该提议放入&lt;strong&gt;投票箱outstandingProposals&lt;/strong&gt;中，同事将该提议广播给所有Follower服务器。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;收集投票&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Follower在接收到Leader发来的提议后，&lt;strong&gt;进入Sync流程记录事务日志&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;一旦日志记录完成后，向Leader服务器发送ACK消息。&lt;/p&gt;

&lt;p&gt;Leader根据ACK消息统计每个提议的投票情况，&lt;strong&gt;当一个提议获得了集群中过半机器的投票，就认为提议通过，进入Commit阶段&lt;/strong&gt;。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;将请求放入toBeApplied队列&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在进入Commit阶段前，ZooKeeper会将请求放入toBeApplied队列中。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;广播Commit消息&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;向Follower发送COMMIT消息，向Observer发送INFORM消息。&lt;/p&gt;

&lt;h4 id=&quot;commit流程&quot;&gt;Commit流程&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;请求交付给CommitProcessor处理器&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;CommitProcessor处理器将请求放入&lt;strong&gt;queuedRequests队列&lt;/strong&gt;中。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;处理queuedRequests队列请求&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;CommitProcessor有&lt;strong&gt;一个单独的线程&lt;/strong&gt;逐个处理queuedRequests队列中的请求。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;标记nextPending&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如果queuedRequests队列中正在处理的是一个事务请求，即需要等待Proposal投票结果(此时正在进行Proposal流程投票)，需要将nextPending标记为当前请求，一方面确保事务请求的顺序性，另一方面便于检测当前是否正在进行事务请求投票。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;等待Proposal投票&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Commit流程中的请求将在queuedRequests处理中等待Proposal流程完成投票。&lt;/p&gt;

&lt;p&gt;当投票通过后(接收到commit消息)，请求将被放入committedRequests队列中，继续Commit流程。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;提交请求&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将请求放入toProcess队列，应用事务。&lt;/p&gt;

&lt;h3 id=&quot;事务应用&quot;&gt;事务应用&lt;/h3&gt;

&lt;p&gt;FinalRequestProcessor处理器将检查请求的有效性并&lt;strong&gt;完成事务在内存数据库中的应用&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;会话响应&quot;&gt;会话响应&lt;/h3&gt;

&lt;p&gt;计算请求在服务端处理花费时间、统计ZXID、lastOp、lastLatency等。&lt;/p&gt;

&lt;p&gt;创建connectResponse响应并发送。&lt;/p&gt;

&lt;h3 id=&quot;事务请求转发&quot;&gt;事务请求转发&lt;/h3&gt;

&lt;p&gt;所有事务请求必须由Leader服务器来处理，但是并不是所有客户端都和Leader服务器保持连接，因此ZooKeeper实现了事务请求转发机制。&lt;/p&gt;

&lt;p&gt;在Follower和Observer中，第一个请求处理器FollowerRequestProcessor和ObserverRequestProcessor，都会检查当前请求是否是事务请求。&lt;/p&gt;

&lt;p&gt;如果是事务请求，&lt;strong&gt;以REQUEST消息形式转发给Leader服务器&lt;/strong&gt;。Leader服务器接受到消息后，解析出客户端的原始请求，提交到自己的请求处理链中进行事务请求处理。&lt;/p&gt;</content><author><name>Cheng Dongping</name></author><summary type="html">服务器角色</summary></entry><entry><title type="html">ZooKeeper的Leader选举机制</title><link href="http://localhost:4000/2019/04/10/zookeeper-leader-election/" rel="alternate" type="text/html" title="ZooKeeper的Leader选举机制" /><published>2019-04-10T00:00:00+08:00</published><updated>2019-04-10T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/10/zookeeper-leader-election</id><content type="html" xml:base="http://localhost:4000/2019/04/10/zookeeper-leader-election/">&lt;p&gt;Leader选举是ZooKeeper中最重要的技术之一，也是保证分布式数据一致性的关键所在。&lt;/p&gt;

&lt;h2 id=&quot;fastleaderelection选举算法&quot;&gt;FastLeaderElection选举算法&lt;/h2&gt;

&lt;h3 id=&quot;vote-投票&quot;&gt;Vote 投票&lt;/h3&gt;

&lt;p&gt;SID：服务器全局唯一ID。&lt;/p&gt;

&lt;p&gt;ZXID：事务ID，唯一表示一次服务器状态的变更。&lt;/p&gt;

&lt;p&gt;一个投票由SID和ZXID组成：(SID, ZXID)。&lt;/p&gt;

&lt;h3 id=&quot;开始投票&quot;&gt;开始投票&lt;/h3&gt;

&lt;p&gt;通常有两种情况导致集群中不存在Leader：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;整个服务器刚刚初始化启动，尚未产生Leader服务器。&lt;/li&gt;
  &lt;li&gt;运行期间当前Leader所在服务器宕机。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此时，集群中所有服务器都处于&lt;strong&gt;LOOKING&lt;/strong&gt;状态，试图选举出Leader。向集群中其他所有机器发出投票(SID, ZXID)。&lt;/p&gt;

&lt;p&gt;第一次投票时，每台自己都投自己。&lt;/p&gt;

&lt;h3 id=&quot;变更投票&quot;&gt;变更投票&lt;/h3&gt;

&lt;p&gt;对接收到的每一个来自其他服务器的投票(vote_sid, vote_zxid)，处理规则如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;先比较zxid，再比较sid。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;如果 vote_zxid &amp;gt; self_zxid, 认可收到的投票，将收到的投票&lt;strong&gt;再次投出&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;如果 vote_zxid &amp;lt; self_zxid, 坚持自己的投票，不做任何改变，不再发出投票。&lt;/li&gt;
  &lt;li&gt;如果 vote_zxid == self_zxid 且 vote_sid &amp;gt; self_sid，认可当前收到的投票，将收到的投票&lt;strong&gt;再次投出&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;如果 vote_zxid == self_zxid 且 vote_sid &amp;lt; self_sid, 坚持自己的投票，不做改变。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_fastLeaderElection_example.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;确定leader&quot;&gt;确定Leader&lt;/h3&gt;

&lt;p&gt;每台机器收到其他机器投票并处理后，都会统计投票状态。&lt;/p&gt;

&lt;p&gt;如果有一台机器收到了超过半数的相同的投票，该投票对应的SID机器即为Leader。&lt;/p&gt;

&lt;h2 id=&quot;leader选举的实现&quot;&gt;Leader选举的实现&lt;/h2&gt;

&lt;h3 id=&quot;投票数据结构&quot;&gt;投票数据结构&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Vote
- id: long // 被推举的Leader的SID值。
- zxid: long // 被推举的Leader的事务ID。
- electionEpoch: long // 当前服务器的选举纪元，每次进入新一轮的投票，该值加一。
- peerEpoch: long // 被推举的Leader的选举纪元。
- state: ServerState // 当前服务器的状态 // LOOKING, FOLLOWING, LEADING, OBSERVING
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;quorumcnxmanager&quot;&gt;QuorumCnxManager&lt;/h3&gt;

&lt;p&gt;QuorumCnxManager负责各台服务器之间的底层Leader选举过程中的网络通信。每台服务器启动时，都会启动一个QuorumCnxManager。&lt;/p&gt;

&lt;p&gt;QuorumCnxManager内部维护一系列的队列，保存接受到的、待发送的消息，以及消息发送器：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;recvQueue&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;消息接收队列，存放从其他服务器接收到的消息。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;queueSendMap&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;消息发送队列，保存待发送的消息。&lt;code class=&quot;highlighter-rouge&quot;&gt;queueSendMap&lt;/code&gt;是一个Map，按照SID进行分组，为每个参与选举的其他服务器单独分配一个队列。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;senderWorkerMap&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;发送器集合，每个SenderWorker对应一台参与选举的其他服务器，负责消息的发送。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;lastMassageSent&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;为每个SID保留最近发送过的一个消息。&lt;/p&gt;

&lt;p&gt;QuorumCnxManager在启动时，会创建ServerSocket监听Leader选举的通信端口(默认: 3888)，并向所有集群中的服务器中&lt;strong&gt;比自己SID小的&lt;/strong&gt;(防止重复连接)发起TCP连接。&lt;/p&gt;

&lt;h3 id=&quot;fastleaderelection算法实现&quot;&gt;FastLeaderElection算法实现&lt;/h3&gt;

&lt;h4 id=&quot;选票管理&quot;&gt;选票管理&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;sendqueue&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;选票发送队列&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;recvqueue&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;选票接收队列&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;WorkerReceiver&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;选票接收器，接收器不断从QuorunCnxManager中获取选举消息，并转化成Vote选票结构，加入recvqueue。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;WorkerSender&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;选票发送器，不断从sendqueue中获取待发送的选票。并传递至底层QuorumCnxManager。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_fastLeaderElection_data_flow.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;选举步骤&quot;&gt;选举步骤&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;自增选举轮次(ElectionEpoch)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ZooKeeper规定所有有效的投票必须在同一轮次。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;初始化选票&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在初始化阶段，每台服务器都推举自己为Leader。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;发送初始化选票&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将刚刚初始化好的选票放入sendqueue队列，由发送器WorkerSender负责发送给所有其他服务器。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;接收外部投票&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;不断地从recvqueue队列中获取外部投票。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;[处理外部投票]判断选举轮次&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;外部投票选举轮次大于内部投票&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;更新本机选举轮次至外部投票轮次，清空recvset中所有已经收到的选票。&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;外部投票选举轮次小于内部投票&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;忽略该外部投票。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;[处理外部投票]选票PK&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;按照ZXID、SID的规则进行PK。如果外部投票获胜，则覆盖内部投票。&lt;/p&gt;

&lt;p&gt;==&lt;strong&gt;并再次将变更后的内部投票发送出去。&lt;/strong&gt;==&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;选票归档&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;每次接受到外部投票都会放入recvset中。&lt;/p&gt;

&lt;p&gt;recvset记录当前服务器在本轮次的Leader选举收到的所有外部投票。一台服务器最多只会存入一张选票，recvset 是一个 HashMap，以外部服务器的 sid 作为 key，它们的选票 Vote 作为 value。&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;recvset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;sid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Vote&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;leader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;zxid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;electionEpoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;peerEpoch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;统计投票&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;每次将外部投票放入recvset中时，都会统计集群中是否已经有过半的服务器认可了当前的内部投票。&lt;/p&gt;

&lt;p&gt;如果已经有过半服务器认可当前的内部投票，则终止投票，否则继续接收，返回步骤4。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;更新服务器状态&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;如统计投票后终止投票，如果当前被认可的服务器是自己，则更新服务器状态为LEADING。 否则根据具体情况更新为FOLLOWING或OBSERVING。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_fastLeaderElection_process.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;refs&quot;&gt;REFS&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;https://juejin.im/post/5cd06b7c51882544da500e36&lt;/li&gt;
  &lt;li&gt;http://www.chilangedu.com/blog/1000001325937566.html&lt;/li&gt;
  &lt;li&gt;https://www.jianshu.com/p/763a5ae127a7&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Cheng Dongping</name></author><summary type="html">Leader选举是ZooKeeper中最重要的技术之一，也是保证分布式数据一致性的关键所在。</summary></entry><entry><title type="html">ZooKeeper的Session机制</title><link href="http://localhost:4000/2019/04/09/zookeeper-session/" rel="alternate" type="text/html" title="ZooKeeper的Session机制" /><published>2019-04-09T00:00:00+08:00</published><updated>2019-04-09T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/09/zookeeper-session</id><content type="html" xml:base="http://localhost:4000/2019/04/09/zookeeper-session/">&lt;p&gt;客户端与服务端之间的任何交互操作都与会话息息相关，包括临时节点的生命周期、客户端请求的顺序执行以及Watcher通知机制等。&lt;/p&gt;

&lt;p&gt;ZooKeeper的连接与会话就是客户端通过实例化ZooKeeper对象来实现客户端与服务器创建并保持TCP连接的过程。&lt;/p&gt;

&lt;h3 id=&quot;会话状态&quot;&gt;会话状态&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;CONNECTING&lt;/li&gt;
  &lt;li&gt;CONNECTED&lt;/li&gt;
  &lt;li&gt;RECONNECTING&lt;/li&gt;
  &lt;li&gt;RECONNECTED&lt;/li&gt;
  &lt;li&gt;CLOSE&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当客户端开始创建ZooKeeper对象时，状态变为CONNECTING。逐个选取服务器地址列表中的IP地址尝试连接，成功连接上后状态变更为CONNECTED。&lt;/p&gt;

&lt;p&gt;伴随着网络问题，客户端与服务器的连接可能断开，此时客户端会自动进行重连，状态再次变更为CONNETING，脸上后又变为CONNECTED，因此，运行期间，客户端的状态是介于CONNECTING和CONNECTED之间。&lt;/p&gt;

&lt;p&gt;会话超时、权限检查失败、客户端主动退出时，客户端状态变更为CLOSE。&lt;/p&gt;

&lt;h3 id=&quot;会话创建&quot;&gt;会话创建&lt;/h3&gt;

&lt;h4 id=&quot;session&quot;&gt;Session&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Session&lt;/strong&gt;是ZooKeeper中的会话实体，包括：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;sessionID 会话全局唯一ID&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;TimeOut 会话超时时间&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;客户端在构造ZooKeeper实例时，向服务器发送配置的SessionTimeout参数，服务器根据自己的超时时间限制确定最终会话超时时间。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;TickTime 下次会话超时时间&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ZooKeeper对会话实行&lt;strong&gt;分桶策略&lt;/strong&gt;，根据当前时间、Timeout计算得出。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;isClosing 会话是否已经关闭&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;当&lt;code class=&quot;highlighter-rouge&quot;&gt;isClosing = true&lt;/code&gt;时，服务器不再处理来自该会话的新请求。&lt;/p&gt;

&lt;h4 id=&quot;sessiontracker&quot;&gt;SessionTracker&lt;/h4&gt;

&lt;p&gt;SessionTracker是ZooKeeper服务端的会话管理器，负责会话的创建、管理和清理工作。其中维护了三个Map：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;HashMap&amp;lt;Long, SessionImpl&amp;gt; &lt;strong&gt;sessionsById&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;根据sessionId得到Session实体。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ConcurrentHashMap&amp;lt;Long, Integer&amp;gt; &lt;strong&gt;sessionsWithTimeout&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;根据sessionID得到会话超时时间。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;HashMap&amp;lt;Long, SessionSet&amp;gt; &lt;strong&gt;sessionSets&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;根据&lt;strong&gt;分桶策略&lt;/strong&gt;管理会话。&lt;/p&gt;

&lt;h4 id=&quot;分桶策略&quot;&gt;分桶策略&lt;/h4&gt;

&lt;p&gt;将&lt;strong&gt;下次超时时间点(ExpirationTime)&lt;/strong&gt;相同的会话放在同一个区块中同一管理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_session_bucket_expiration_management.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ZooKeeper Leader服务器每隔&lt;strong&gt;ExpirationInterval&lt;/strong&gt;会检查会清理超时会话，为了方便多多个会话同时检查，&lt;strong&gt;ExpirationTime&lt;/strong&gt;是&lt;strong&gt;ExpirationInterval&lt;/strong&gt;的整数倍：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ExpirationTime_ = CurrentTime + SessionTimeout
ExpirationTime = (ExpirationTime_/ExpirationInterval + 1) * ExpirationInterval
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;会话管理&quot;&gt;会话管理&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;会话激活 TouchSession (保持会话的有效性)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/Users/cdp/Work/ImageHostInGithub/zookeeper_session_touch.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;每当客户端向服务端发送请求，会触发会话激活。&lt;/li&gt;
  &lt;li&gt;客户端在&lt;code class=&quot;highlighter-rouge&quot;&gt;sessionTimeout/3&lt;/code&gt;时间内未和服务器进行过任何通信时，客户端会向服务端发送PING请求。&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;会话超时检查&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;SessionTracker中单独的线程进行会话超时检查。&lt;/p&gt;

&lt;p&gt;线程每隔ExpirationInterval时间，逐个依次地对会话桶中剩下的会话进行清理。&lt;/p&gt;

&lt;h4 id=&quot;会话清理&quot;&gt;会话清理&lt;/h4&gt;

&lt;p&gt;SessionTracker超时检查线程会对已经过期的会话进行会话清理。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;标记isClosing = true。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;提交“会话关闭”请求，使会话关闭操作在整个服务端集群中都生效。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;收集需要清理的临时节点。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ZooKeeper内存数据库中维护了“会话ID – 临时节点集合”的表。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;删除临时节点&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;逐个将这些临时界定啊转换成“节点删除”请求并删除会话对应的所有临时节点。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;移除会话&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从SessionTracker中得sessionsById、sessionsWithTimeout和sessionSets中移除会话。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;关闭 NIOServerCnxn&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;重连&quot;&gt;重连&lt;/h4&gt;

&lt;p&gt;客户端和服务端网络连接断开时，客户端会自动反复重连，再次连接上的客户端状态可能是：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CONNECTED&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在会话超时时间内重新连接上集群中任意一台机器。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;EXPIRED&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在会话超时时间外重新连接上。&lt;/p&gt;

&lt;p&gt;当客户端与服务器端连接出现问题断开时，客户端可能出现的异常有：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CONNECTION_LOSS (org.apache.zookeeper.KeeperException$ConnectionLossException)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;场景：&lt;/p&gt;

&lt;p&gt;客户端在请求服务端时，网络异常，客户端会立即接受到事件None-Disconnected通知，并抛出ConnectionLossException。&lt;/p&gt;

&lt;p&gt;应对策略：&lt;/p&gt;

&lt;p&gt;应用程序应该捕获该异常，并等待客户端自动完成重连，成功重连后客户端会受到None-SyncConnected通知，此时可以重新发送请求。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SESSION_EXPIRED&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;场景：&lt;/p&gt;

&lt;p&gt;在CONNECTION_LOSS期间，由于重连耗时过长，超过了会话超时时间(sessionTimeout)，成功重连后，服务器会告知客户端会话超时(SESSION_EXPIRED)。&lt;/p&gt;

&lt;p&gt;应对策略：&lt;/p&gt;

&lt;p&gt;用户需要重新实例化一个ZooKeeper对象。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;SESSION_MOVED&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;场景：&lt;/p&gt;

&lt;p&gt;客户端和服务器S1之间的连接断开后，CONNECTION_LOSS期间，客户端重新连接了新的服务器S2。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;可能出现的问题：
1. C1向S1发送请求R1 setData(/app/data, 1);
2. 请求R1到达S1之前，C1与S1之间的连接断开，并重连上S2。
3. C1向S2发送请求R2 setData(/app/data, 2);
4. S2处理了R2后，此时R1到达S1，对于客户端来说，正确的请求R2被错误的请求R1覆盖。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;应对策略：&lt;/p&gt;

&lt;p&gt;服务端在处理客户端请求的时候，会首先检查会话所有者，如果所有者不是当前服务器，则向客户端发出SessionMovedException。&lt;/p&gt;</content><author><name>Cheng Dongping</name></author><summary type="html">客户端与服务端之间的任何交互操作都与会话息息相关，包括临时节点的生命周期、客户端请求的顺序执行以及Watcher通知机制等。</summary></entry><entry><title type="html">ZooKeeper客户端实现简介</title><link href="http://localhost:4000/2019/04/08/zookeeper-client/" rel="alternate" type="text/html" title="ZooKeeper客户端实现简介" /><published>2019-04-08T00:00:00+08:00</published><updated>2019-04-08T00:00:00+08:00</updated><id>http://localhost:4000/2019/04/08/zookeeper-client</id><content type="html" xml:base="http://localhost:4000/2019/04/08/zookeeper-client/">&lt;h2 id=&quot;客户端&quot;&gt;客户端&lt;/h2&gt;

&lt;p&gt;ZooKeeper的客户端主要由以下几个核心组件组成：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ZooKeeper实例：客户端的入口。&lt;/li&gt;
  &lt;li&gt;ClientWatchManager：客户端Watcher管理器。&lt;/li&gt;
  &lt;li&gt;HostProvider：客户端地址列表管理器。&lt;/li&gt;
  &lt;li&gt;ClientCnxn：客户端核心线程，包括SendThread和EventThread。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_client_structure.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;zookeeper初始化及会话流程&quot;&gt;ZooKeeper初始化及会话流程&lt;/h3&gt;

&lt;p&gt;ZooKeeper实例的创建过程，就是ZooKeeper客户端的初始化与气动环节。&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ZooKeeper客户端的构造方法：
ZooKeeper(String connectString, int sessionTimeout, Watcher watcher);
ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, boolean canBeReadOnly);
ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, long sessionId, byte[] sessionPasswd);
ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, long sessionId, byte[] sessionPasswd, boolean canBeReadOnly);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;初始化阶段&quot;&gt;初始化阶段&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;初始化ZooKeeper对象&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;构造方法实例化ZooKeeper对象。&lt;/li&gt;
  &lt;li&gt;创建客户端Watcher管理器ClientWatchManager。&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;设置默认Watcher&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;将构造方法传入的Watcher对象作为defaultWatcher存入ClientWatchManager。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;构造==服务器地址列表管理器HostProvider==&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;解析connectString, 将传入的服务器地址保存在HostProvider。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;初始化客户端网路连接器 ClientCnxn&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;CLientCnxn负责客户端与服务器的网路交互：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;outgoingQueue + pendingQueue：客户端请求发送队列 + 服务端响应等待队列。&lt;/li&gt;
  &lt;li&gt;SendThread：负责客户端和服务器端之间的所有网络I/O。&lt;/li&gt;
  &lt;li&gt;EventThread：负责事件处理。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;会话创建阶段&quot;&gt;会话创建阶段&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;启动SendThread和EventThread。&lt;/li&gt;
  &lt;li&gt;从HostProvider随机获取一个地址，创建TCP连接。&lt;/li&gt;
  &lt;li&gt;SendThread发出连接请求ConnectRequest。&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;响应处理阶段&quot;&gt;响应处理阶段&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;ClientCnxn接收服务端响应。&lt;/li&gt;
  &lt;li&gt;处理Response，通知SendThread及HostProvider连接成功。&lt;/li&gt;
  &lt;li&gt;生成SyncConencted-None连接成功事件。&lt;/li&gt;
  &lt;li&gt;EventThread查询SyncConnected-None对应的Watcher并执行。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;服务器地址解析&quot;&gt;服务器地址解析&lt;/h3&gt;

&lt;h5 id=&quot;connectstringparser&quot;&gt;ConnectStringParser&lt;/h5&gt;

&lt;p&gt;在构造ZooKeeper实例时，传入的connectString是一个服务器地址列表：&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// 所有地址在一个字符串上，使用英文逗号分隔
192.168.0.1:2181,192.168.0.2:2181/apps/X,192.168.0.3:2181/apps/X
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;ZooKeeper客户端内部接收到服务器地址列表后，封装至ConnectStringParser对象中：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-Java&quot;&gt;public final class ConnectStringParser {
    String chrootPath;
    ArrayList&amp;lt;InetSocketAddress&amp;gt; serverAddresses = new ArrayList&amp;lt;InetSocketAddress&amp;gt;();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;chrootPath是客户端隔离命令空间，用来设置应用的根目录。一旦设置了Chroot之后，客户端和ZooKeeper服务器发起的所有请求中相关的节点路径，都是一个相对路径，根路径就是Chroot。&lt;/p&gt;

&lt;p&gt;serverAddresses保存了所有设置的服务器的IP和Port。&lt;/p&gt;

&lt;h5 id=&quot;hostprovider-地址列表管理器&quot;&gt;HostProvider 地址列表管理器&lt;/h5&gt;

&lt;p&gt;客户端通过HostProvider从地址列表中选择连接服务器。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public interface HostProvider{
    public int size(); // 返回当前服务器地址列表个数
    public InetSocketAddress next(long spinDelay); // 选择一个服务器地址并返回
    public void onConnected(); // 如果客户端与服务器成功创建连接，会调用该回调方法
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;StaticHostProvider&lt;/strong&gt;是HostProvider接口的默认实现。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;StaticHostProvider&lt;/strong&gt;首先将服务器地址列表随机排列后组织成一个环形列表，之后就一直按照该环形顺序获取服务器地址。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/cheng-dp/ImageHostInGithub/master/zookeeper_staticHostProvider_next.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cientcnxn-网络io&quot;&gt;CientCnxn 网络I/O&lt;/h3&gt;

&lt;p&gt;ClientCnxn是客户端的核心工作类，负责维护客户端与服务端之间的网络通信。&lt;/p&gt;

&lt;h4 id=&quot;packet&quot;&gt;Packet&lt;/h4&gt;

&lt;p&gt;Packet是ClientCnxn内部定义的对ZooKeeper协议层的封装，是ZooKeeper中请求与响应的载体。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Packet
- requestHeader: RquestHeader // 请求头
- replyHeader: ReplyHeader // 响应头
- request: Record // 请求体
- response: Record // 响应体
- bb: ByteBuffer
- clientPath: String // 节点路径
- serverPath: String // 节点路径
- finished: boolean
- cb: AsyncCallback
- ctx: Object
- watchRegistration: WatchRegistration // 注册的Watcher
- readOnly: boolean
-------------------------
+ createBB(): void
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;只有&lt;code class=&quot;highlighter-rouge&quot;&gt;requestHeader&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;request&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;readOnly&lt;/code&gt;会被发送给服务端，其余都直接保存在客户端的上下文中。&lt;/p&gt;

&lt;h4 id=&quot;outgoingqueue和pendingqueue&quot;&gt;outgoingQueue和pendingQueue&lt;/h4&gt;

&lt;p&gt;outgoingQueue：&lt;/p&gt;

&lt;p&gt;存储需要发送到服务端的Packet集合。&lt;/p&gt;

&lt;p&gt;pendingQueue:&lt;/p&gt;

&lt;p&gt;存储已经从客户端发送到服务端，但是需要等待服务端响应的Packet集合。&lt;/p&gt;

&lt;h4 id=&quot;sendthread&quot;&gt;SendThread&lt;/h4&gt;

&lt;p&gt;SendThread是客户端ClientCnxn的核心I/O调度线程。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;==维护客户端与服务端的会话生命周期。==&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;周期地向服务端发送PING包实现心跳检测。&lt;/li&gt;
  &lt;li&gt;如果和服务端的TCP连接断开，自动且透明化地完成重连。&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;管理客户端所有的请求发送和响应接收操作。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;负责传递服务端的事件给EventThread。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;eventthread&quot;&gt;EventThread&lt;/h4&gt;

&lt;p&gt;负责客户端事件处理，触发客户端注册的Watcher监听回调。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;维护watingEvents队列，包括客户端注册的Watcher和异步接口中注册的AsyncCallback。&lt;/li&gt;
  &lt;li&gt;不断从watingEvents队列中取出Object，根据类型(Watcher/AsyncCallback)执行回调。&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Cheng Dongping</name></author><summary type="html">客户端</summary></entry></feed>